<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RFC-038: Local Inference Integration</title>
  <link rel="stylesheet" href="rfc.css">
</head>
<body>
<h1>RFC-038: Local Inference Integration</h1>
<dl class="metadata">
</dl>
<hr>
<section>
<h2>Abstract</h2>
<p>This RFC specifies how Library of Cyberspace agents integrate with local large language model (LLM) inference backends. Local inference enables privacy-preserving agent operations without external API dependencies, supporting the self-sovereign architecture of the Library.[^h1]</p>
<p>[^h1]: Historical: The tension between local and remote computation echoes the mainframe-to-PC transition. Local inference returns agency to the edge, reversing decades of cloud centralization.</p>
</section>
<section>
<h2>Motivation</h2>
<p>Agents in the Library of Cyberspace require language understanding and generation capabilities for:</p>
<ul>
<li>Document summarization and indexing</li>
<li>Natural language query translation (RFC-025)</li>
<li>Content annotation and metadata extraction</li>
<li>Inter-agent communication in natural language</li>
</ul>
<p>External API dependencies (OpenAI, Anthropic, etc.) introduce:</p>
<p>1. Privacy leakage — document content leaves the realm 2. Availability risk — network partitions break agent operation 3. Cost unpredictability — metered APIs scale poorly 4. Vendor lock-in — proprietary formats and rate limits</p>
<p>Local inference eliminates these concerns while maintaining capability.[^d1]</p>
<p>[^d1]: Design: We deliberately avoid specifying model architectures. The interface is model-agnostic—agents negotiate capabilities at runtime.</p>
</section>
<section>
<h2>Architecture</h2>
<h3>Inference Topology</h3>
<p>Table 1: Inference Deployment Models</p>
<table>
<tr><th>Model </th><th>Description </th><th>Use Case </th></tr>
<tr><td>Realm-local </td><td>Inference server within realm </td><td>Default, privacy-preserving </td></tr>
<tr><td>Node-local </td><td>Per-node inference </td><td>Edge agents, mobile </td></tr>
<tr><td>Federated </td><td>Shared across trusted realms </td><td>Resource pooling </td></tr>
</table>
<pre>
┌─────────────────────────────────────────────────────┐
│                      Realm                          │
│  ┌─────────┐    ┌──────────────┐    ┌───────────┐  │
│  │  Agent  │───▶│   Inference  │───▶│   Model   │  │
│  │         │◀───│    Server    │◀───│  Weights  │  │
│  └─────────┘    └──────────────┘    └───────────┘  │
│       │              :11434              │          │
│       ▼                                  ▼          │
│  ┌─────────┐                      ┌───────────┐    │
│  │  Vault  │                      │   VRAM    │    │
│  └─────────┘                      └───────────┘    │
└─────────────────────────────────────────────────────┘
</pre>
<h3>Protocol</h3>
<p>Agents communicate with inference servers via HTTP REST API.[^i1]</p>
<p>[^i1]: Implementation: Ollama exposes an OpenAI-compatible API at /v1/chat/completions and a native API at /api/generate. We specify both for maximum compatibility.</p>
<p>Discovery:</p>
<pre class="language-scheme">
(define (discover-inference-server)
  ;; Check well-known locations
  (or (probe "http://localhost:11434")      ; Ollama default
      (probe "http://localhost:8080")       ; llama.cpp
      (probe (realm-config 'inference-url)) ; Configured
      #f))                                   ; None available
</pre>
<p>Capability Negotiation:</p>
<pre class="language-scheme">
(define (negotiate-capabilities server)
  (let ((models (http-get (string-append server "/api/tags"))))
    (map (lambda (m)
           `((name . ,(alist-ref 'name m))
             (parameters . ,(alist-ref 'size m))
             (context-length . ,(model-context-length m))
             (capabilities . ,(model-capabilities m))))
         models)))
</pre>
<p>Table 2: Model Capabilities</p>
<table>
<tr><th>Capability </th><th>Description </th><th>Example Models </th></tr>
<tr><td>completion </td><td>Text generation </td><td>All </td></tr>
<tr><td>chat </td><td>Multi-turn conversation </td><td>Llama 3, Mistral </td></tr>
<tr><td>embedding </td><td>Vector embeddings </td><td>nomic-embed, mxbai </td></tr>
<tr><td>code </td><td>Code generation </td><td>CodeLlama, DeepSeek </td></tr>
<tr><td>vision </td><td>Image understanding </td><td>LLaVA, Llama 3.2 Vision </td></tr>
</table>
</section>
<section>
<h2>Agent Integration</h2>
<h3>Scribe Agent</h3>
<p>The scribe agent uses local inference for document processing:[^r1]</p>
<p>[^r1]: Research: Retrieval-augmented generation (RAG) combines local inference with vault content retrieval. See Lewis et al., "Retrieval- Augmented Generation for Knowledge-Intensive NLP Tasks" (2020).</p>
<pre class="language-scheme">
(define (scribe-summarize document)
  (let ((server (discover-inference-server))
         (model (select-model server 'completion))
         (prompt (format "Summarize the following document:\n\n~a"
                         (document-content document))))
    (inference-complete server model prompt
                        '((max-tokens . 500)
                          (temperature . 0.3)))))

(define (scribe-index document)
  ;; Extract keywords using local inference
  (let ((server (discover-inference-server))
         (model (select-model server 'completion))
         (prompt (format "Extract 5-10 keywords from:\n\n~a"
                         (document-content document))))
    (parse-keywords (inference-complete server model prompt))))
</pre>
<h3>Query Translation</h3>
<p>Natural language queries translate to RFC-025 query language:</p>
<pre class="language-scheme">
(define (nl-to-query natural-language)
  (let* ((server (discover-inference-server))
         (model (select-model server 'chat))
         (system "You translate natural language to Cyberspace query
                  language. Output only the query, no explanation.")
         (examples '(("find all RFCs about security"
                      "(query (type rfc) (topic security))")
                     ("documents modified last week"
                      "(query (modified (after (days-ago 7))))"))))
    (inference-chat server model system examples natural-language)))
</pre>
<h3>Demonic Agent Inference</h3>
<p>Sandboxed agents (RFC-023) access inference through capability tokens:[^d2]</p>
<p>[^d2]: Design: Inference capability is granted like any other—via SPKI certificate. An agent cannot infer without explicit authorization.</p>
<pre class="language-scheme">
(define (demonic-inference agent prompt)
  (let ((cap (agent-capability agent 'inference)))
    (if (not cap)
        (error 'unauthorized "Agent lacks inference capability")
        (let ((limits (capability-limits cap)))
          (enforce-limits limits)
          (inference-complete (capability-server cap)
                              (capability-model cap)
                              prompt
                              limits)))))
</pre>
</section>
<section>
<h2>Resource Management</h2>
<h3>VRAM Allocation</h3>
<p>Table 3: Model Size Guidelines</p>
<table>
<tr><th>Parameters </th><th>VRAM (Q4) </th><th>VRAM (FP16) </th><th>Context </th></tr>
<tr><td>7B </td><td>4 GB </td><td>14 GB </td><td>8K </td></tr>
<tr><td>13B </td><td>8 GB </td><td>26 GB </td><td>8K </td></tr>
<tr><td>34B </td><td>20 GB </td><td>68 GB </td><td>16K </td></tr>
<tr><td>70B </td><td>40 GB </td><td>140 GB </td><td>32K </td></tr>
</table>
<h3>Rate Limiting</h3>
<p>Local inference is limited by hardware, not API quotas. Realms implement fair scheduling across agents:[^i2]</p>
<p>[^i2]: Implementation: Token bucket algorithm with per-agent quotas. Prevents single agent from monopolizing inference resources.</p>
<pre class="language-scheme">
(define (inference-rate-limit agent tokens)
  (let ((bucket (agent-token-bucket agent)))
    (if (bucket-consume bucket tokens)
        #t
        (begin
          (agent-wait agent (bucket-refill-time bucket))
          (inference-rate-limit agent tokens)))))
</pre>
<h3>Fallback Behavior</h3>
<p>When local inference is unavailable:</p>
<p>1. Queue — buffer requests for later processing 2. Degrade — use simpler heuristics (keyword extraction vs. LLM) 3. Federate — request inference from trusted peer realm 4. Fail — return error to requesting agent</p>
<pre class="language-scheme">
(define (inference-with-fallback server model prompt)
  (or (try-inference server model prompt)
      (try-queued-inference prompt)
      (try-degraded-processing prompt)
      (try-federated-inference prompt)
      (error 'inference-unavailable)))
</pre>
</section>
<section>
<h2>Security Considerations</h2>
<h3>Prompt Injection</h3>
<p>Agents MUST sanitize document content before inclusion in prompts:[^r2]</p>
<p>[^r2]: Research: Prompt injection attacks embed malicious instructions in user content. See Perez &amp; Ribeiro, "Ignore This Title and HackAPrompt" (2023).</p>
<pre class="language-scheme">
(define (safe-prompt template document)
  (let ((sanitized (sanitize-for-prompt (document-content document))))
    (format template sanitized)))

(define (sanitize-for-prompt text)
  ;; Remove instruction-like patterns
  (regexp-replace-all
    '("ignore previous" "disregard" "new instructions" "system:")
    text
    "[REDACTED]"))
</pre>
<h3>Model Provenance</h3>
<p>Realms SHOULD verify model checksums before loading:[^d3]</p>
<p>[^d3]: Design: Model weights can contain backdoors. Verification against known-good checksums prevents supply chain attacks.</p>
<pre class="language-scheme">
(define (verify-model model-path expected-hash)
  (let ((actual-hash (sha256-file model-path)))
    (unless (equal? actual-hash expected-hash)
      (error 'model-verification-failed
             "Model checksum mismatch"))))
</pre>
<h3>Inference Isolation</h3>
<p>Sensitive documents require isolated inference contexts:</p>
<ul>
<li>Separate model instances per security domain</li>
<li>Clear KV cache between requests from different agents</li>
<li>No persistent memory across security boundaries</li>
</ul>
</section>
<section>
<h2>Ollama Integration</h2>
<p>Ollama is the reference implementation for local inference.[^h2]</p>
<p>[^h2]: Historical: Ollama follows the Unix philosophy—do one thing well. It manages models and serves inference. Nothing more.</p>
<h3>Installation</h3>
<pre class="language-bash">
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Verify
ollama --version
</pre>
<h3>Model Management</h3>
<pre class="language-bash">
# Pull models
ollama pull llama3
ollama pull nomic-embed-text

# List available
ollama list

# Remove
ollama rm unused-model
</pre>
<h3>API Endpoints</h3>
<p>Table 4: Ollama API Endpoints</p>
<table>
<tr><th>Endpoint </th><th>Method </th><th>Purpose </th></tr>
<tr><td>/api/generate </td><td>POST </td><td>Text completion </td></tr>
<tr><td>/api/chat </td><td>POST </td><td>Multi-turn chat </td></tr>
<tr><td>/api/embeddings </td><td>POST </td><td>Vector embeddings </td></tr>
<tr><td>/api/tags </td><td>GET </td><td>List models </td></tr>
<tr><td>/api/show </td><td>POST </td><td>Model details </td></tr>
</table>
<h3>Scheme Bindings</h3>
<pre class="language-scheme">
(define ollama-base "http://localhost:11434")

(define (ollama-generate model prompt #!key (options '()))
  (http-post (string-append ollama-base "/api/generate")
             ((model . ,model)
               (prompt . ,prompt)
               (stream . #f)
               ,@options)))

(define (ollama-chat model messages #!key (options '()))
  (http-post (string-append ollama-base "/api/chat")
             ((model . ,model)
               (messages . ,messages)
               (stream . #f)
               ,@options)))

(define (ollama-embed model text)
  (http-post (string-append ollama-base "/api/embeddings")
             `((model . ,model)
               (prompt . ,text))))
</pre>
</section>
<section>
<h2>Compatibility</h2>
<h3>Alternative Backends</h3>
<p>The protocol supports any OpenAI-compatible inference server:</p>
<ul>
<li>llama.cpp — server binary with --host flag - vLLM — production serving with PagedAttention</li>
<li>LocalAI — drop-in OpenAI replacement</li>
<li>LM Studio — GUI with server mode</li>
</ul>
<h3>Cloud Fallback</h3>
<p>For realms without local GPU resources, cloud inference MAY be used as fallback with explicit user consent and encryption:</p>
<pre class="language-scheme">
(define (cloud-inference-fallback prompt)
  (when (user-consent? 'cloud-inference)
    (let ((encrypted (encrypt-for-cloud prompt)))
      ;; Use cloud API with encrypted prompt
      ;; Decrypt response locally
      )))
</pre>
<p>This is NOT RECOMMENDED for sensitive documents.</p>
</section>
<section>
<h2>References</h2>
<p>1. RFC-023: Demonic Agent Sandboxing 2. RFC-025: Query Language 3. RFC-035: Mobile Agents and Pub/Sub 4. Ollama Documentation: https://ollama.com/ 5. Lewis et al., "Retrieval-Augmented Generation" (2020)</p>
</section>
<section>
<h2>Changelog</h2>
<ul>
<li>2026-01-07</li>
<li>Initial specification</li>
</ul>
</section>
</body>
</html>
