<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RFC-039: Scaling Architecture for IPv6</title>
  <link rel="stylesheet" href="rfc.css">
</head>
<body>
<h1>RFC-039: Scaling Architecture for IPv6</h1>
<dl class="metadata">
</dl>
<hr>
<section>
<h2>Abstract</h2>
<p>This RFC defines the architectural changes required to scale Cyberspace from a git-backed prototype to a native distributed system capable of operating at IPv6 scale (billions of realms, exabytes of content). Git becomes an export format; the vault becomes the source of truth.</p>
</section>
<section>
<h2>Terminology</h2>
<p>Realm: A node's place in cyberspace - its vault, principal, capabilities, and objects. Each realm is sovereign: local-first, controlled by its operator. Realms federate by choice, sharing objects according to trust relationships.</p>
<p>Vault: The local content-addressed object store (.vault/). The vault IS the realm's storage - all objects, catalogs, audit trails, and configuration live here.</p>
<p>Principal: A node's cryptographic identity (Ed25519 public key). The principal identifies the realm to peers and signs its objects.</p>
<p>At IPv6 scale, cyberspace consists of billions of realms, each occupying its own address space, each sovereign, each choosing what to share and with whom.</p>
</section>
<section>
<h2>Motivation</h2>
<p>Git served as an excellent prototype substrate: - Content-addressed objects (proof of concept) - Merkle tree integrity (validates the model) - Ubiquitous tooling (bootstrap adoption)</p>
<p>Git cannot scale to IPv6: - Full history on every clone - Repository as replication unit (too coarse) - SHA-1 (cryptographically broken) - No native federation or discovery - Merge semantics are wrong model</p>
<p>The internet has 2^128 addresses. Cyberspace should use them.</p>
</section>
<section>
<h2>Design Principles</h2>
<pre>
1. Objects, not repositories
2. Pull, not push
3. Lazy, not eager
4. Local-first, federate-second
5. Trust math, not infrastructure
</pre>
</section>
<section>
<h2>Content-Addressed Object Store</h2>
<h3>Storage Model</h3>
<pre>
.vault/
  objects/
    sha512-a1b2c3.../    # First 8 chars as directory
      a1b2c3d4e5f6...    # Full hash as filename (S-expression)
  catalog/
    manifest.sexp        # Vault catalog object
    bloom.sexp           # Bloom filter object
    indices/             # Secondary index objects
      by-signer.sexp
      by-type.sexp
  chunks/
    sha512-xxxx/         # Chunked large objects
  audit/
    head.sexp            # Current audit chain head
    chain/               # Audit entries (hash-addressed)
</pre>
<h3>Object Format</h3>
<pre class="language-scheme">
(cyberspace-object
  (version 1)
  (type blob|tree|manifest|cert|audit)
  (size 1048576)
  (compression zstd|none)
  (hash "sha512:a1b2c3...")
  (chunks ("sha512:..." "sha512:..." ...))  ; If chunked
  (signature "ed25519:...")
  (timestamp 1736300000))
</pre>
<h3>Chunking Strategy</h3>
<p>Large objects split at content-defined boundaries (Rabin fingerprinting):</p>
<pre>
Target chunk: 64 KB (Starlink-optimized)
Min chunk:    16 KB
Max chunk:   256 KB

Benefits:
  - Deduplication across objects
  - Partial sync (fetch only missing chunks)
  - Resumable transfers
  - Efficient diff
</pre>
<h3>Hash Function</h3>
<pre>
SHA-512 everywhere.

Not SHA-256: We have the bits, use them.
Not SHA-1: Broken.
Not BLAKE3: Less analyzed, marginal speed gain irrelevant at network latency.

SHA-512 is:
  - FIPS certified (GovCloud path)
  - 50 years of cryptanalysis
  - Hardware accelerated
  - Already in use (audit trail, signatures)
</pre>
</section>
<section>
<h2>Catalog and Query</h2>
<p>The soup IS the catalog. No SQL. Objects are S-expressions, queries are pattern matching.</p>
<h3>The Soup Query Model</h3>
<pre class="language-scheme">
;; Find objects by pattern
(soup-query
  '(cyberspace-object
    (type blob)
    (signer "ed25519:alice...")
    ?rest))                        ; Match any object signed by Alice

;; Find by hash (direct lookup)
(soup-fetch "sha512:a1b2c3...")    ; O(1) content-addressed

;; Find by type
(soup-query '(cyberspace-object (type cert) ?rest))

;; Find by time range
(soup-query-range
  type: 'audit
  from: 1736000000
  to:   1736100000)

;; Cursor-based iteration for large result sets
(soup-cursor
  '(cyberspace-object (type ?) ?rest)
  batch: 100)
</pre>
<h3>Object Catalog</h3>
<p>The catalog is itself an object in the soup - a manifest of what the vault contains:</p>
<pre class="language-scheme">
(vault-catalog
  (version 1)
  (realm "ed25519:principal...")
  (object-count 150000)
  (types
    (blob 100000)
    (tree 30000)
    (cert 15000)
    (audit 5000))
  (bloom-filter #${...})           ; Fast existence check
  (updated 1736400000))
</pre>
<h3>Bloom Filter</h3>
<p>Fast existence check before network round-trip:</p>
<pre class="language-scheme">
(define (soup-maybe-contains? hash)
  "Check bloom filter - false means definitely not, true means maybe"
  (let ((catalog (soup-fetch-catalog)))
    (bloom-test (catalog-bloom catalog) hash)))

;; Bloom parameters
(bloom-filter
  (capacity 10000000)              ; 10M objects
  (false-positive 0.001)           ; 0.1% FP rate
  (bits #${...}))
</pre>
<h3>Audit Trail</h3>
<p>The audit trail is a hash-chain of objects in the soup:</p>
<pre class="language-scheme">
(audit-entry
  (sequence 12345)
  (timestamp 1736300000)
  (lamport 67890)
  (actor "ed25519:subject...")
  (action (read "sha512:object..."))
  (previous "sha512:prev-entry...")  ; Chain link
  (signature "ed25519:auditor..."))

;; Query audit by walking the chain
(define (audit-query actor from-seq)
  "Walk audit chain, filter by actor"
  (soup-chain-walk
    start: (audit-head)
    filter: (lambda (entry)
              (equal? (entry-actor entry) actor))
    from: from-seq))
</pre>
<h3>Secondary Indices</h3>
<p>For queries that can't use content-addressing, the soup maintains lightweight indices as objects:</p>
<pre class="language-scheme">
(soup-index
  (name "by-signer")
  (key-type principal)
  (entries
    (("ed25519:alice..." ("sha512:obj1" "sha512:obj2" ...))
     ("ed25519:bob..." ("sha512:obj3" "sha512:obj4" ...)))))

(soup-index
  (name "by-type")
  (key-type symbol)
  (entries
    ((blob ("sha512:..." "sha512:..." ...))
     (cert ("sha512:..." "sha512:..." ...))
     (audit ("sha512:..." "sha512:..." ...)))))
</pre>
<p>Indices are rebuilt on demand, not authoritative - the soup is truth.</p>
</section>
<section>
<h2>Discovery and Routing</h2>
<h3>Realm Identity</h3>
<p>Each realm has a principal (Ed25519 public key). This IS its identity:</p>
<pre class="language-scheme">
(realm-identity
  (principal "ed25519:a1b2c3...")
  (addresses                          ; Where to reach this realm
    (ipv6 "2001:db8::1" port: 7777)
    (ipv4 "192.0.2.1" port: 7777)     ; Legacy
    (onion "xxxx.onion" port: 7777))  ; Tor
  (role witness)
  (capabilities (storage-gb 1000) (bandwidth-mbps 100))
  (signature "ed25519:..."))
</pre>
<h3>Peer Discovery</h3>
<p>Bootstrap:</p>
<pre class="language-scheme">
(bootstrap-peers
  ("ed25519:official1..." "bootstrap.cyberspace.org")
  ("ed25519:official2..." "bootstrap2.cyberspace.org"))
</pre>
<p>Gossip Protocol:</p>
<pre>
1. Realm joins, contacts bootstrap peer
2. Receives partial peer list (random subset)
3. Contacts those peers, exchanges lists
4. Epidemic spread: O(log n) rounds to reach all realms
5. Periodic refresh (every 5 min on Starlink-friendly schedule)
</pre>
<p>Distributed Hash Table (Future):</p>
<pre>
Kademlia-style routing:
  - XOR distance metric on principal hashes
  - O(log n) lookups
  - Realms responsible for nearby hash ranges
  - Natural load balancing
</pre>
<h3>Content Discovery</h3>
<pre class="language-scheme">
;; "Who has this hash?"
(content-locate "sha512:a1b2c3...")

;; Returns list of peers claiming to have it:
(("ed25519:peer1..." (latency-ms 50) (role full))
 ("ed25519:peer2..." (latency-ms 200) (role witness))
 ("ed25519:peer3..." (latency-ms 600) (role archiver)))

;; Fetch from best candidate
(content-fetch "sha512:a1b2c3..." from: "ed25519:peer1...")
</pre>
</section>
<section>
<h2>Transport Protocol</h2>
<h3>End-to-End Encryption</h3>
<p>All network traffic is encrypted. The OS and network are not trusted.</p>
<pre class="language-scheme">
(cyberspace-message
  (version 1)
  (from "ed25519:sender...")
  (to "ed25519:recipient...")        ; Or broadcast key
  (ephemeral "x25519:...")           ; One-time key (PFS)
  (nonce #${24-bytes})               ; Random nonce
  (ciphertext #${...})               ; NaCl box: X25519 + XSalsa20-Poly1305
  (signature "ed25519:..."))         ; Sign the ciphertext
</pre>
<p>Encryption scheme: NaCl crypto_box (libsodium) - Key agreement: X25519 (Curve25519 ECDH) - Cipher: XSalsa20-Poly1305 - Perfect forward secrecy: ephemeral keys per message</p>
<p>Decrypted payload:</p>
<pre class="language-scheme">
(plaintext-payload
  (type request|response|announce|gossip)
  (nonce 12345678)                   ; Replay protection
  (timestamp 1736300000)
  (body ...))
</pre>
<p>Broadcast messages use a shared group key or are signed-only (announcements of public objects).</p>
<h3>Wire Format (Encrypted)</h3>
<pre class="language-scheme">
;; Sender encrypts
(define (seal-message payload recipient-pubkey sender-keypair)
  (let ((ephemeral (x25519-keypair))
         (shared (x25519-shared (ephemeral-secret ephemeral) recipient-pubkey))
         (nonce (random-bytes 24))
         (ciphertext (crypto-box payload nonce shared)))
    `(cyberspace-message
      (version 1)
      (from ,(keypair-public sender-keypair))
      (to ,recipient-pubkey)
      (ephemeral ,(ephemeral-public ephemeral))
      (nonce ,nonce)
      (ciphertext ,ciphertext)
      (signature ,(sign-message ciphertext sender-keypair)))))

;; Recipient decrypts
(define (open-message msg recipient-keypair)
  (let ((shared (x25519-shared (keypair-secret recipient-keypair)
                                (message-ephemeral msg)))
         (plaintext (crypto-box-open (message-ciphertext msg)
                                     (message-nonce msg)
                                     shared)))
    (and (verify-signature msg (message-from msg))
         plaintext)))
</pre>
<h3>Request Types</h3>
<pre class="language-scheme">
;; Existence check
(have? ("sha512:..." "sha512:..." ...))
;; Response: (have ("sha512:..." "sha512:...") missing ("sha512:..."))

;; Fetch object
(fetch "sha512:...")
;; Response: (object ...)

;; Fetch chunk range
(fetch-chunks "sha512:..." start: 5 count: 10)
;; Response: (chunks ...)

;; Peer list exchange
(peers? limit: 50)
;; Response: (peers ...)

;; Announce new content
(announce ("sha512:..." "sha512:..."))
;; Response: (ack)
</pre>
<h3>Transport Bindings</h3>
<pre>
Native:     UDP/IPv6, port 7777 (primary)
Fallback:   TCP/IPv6, port 7777 (firewalls)
Legacy:     TCP/IPv4, port 7777 (transition)
Stealth:    Tor onion service (censorship resistance)
Offline:    USB drive, file copy (sneakernet)
Export:     Git bundle (GitHub compatibility)
</pre>
<h3>Starlink Optimization</h3>
<pre class="language-scheme">
(transport-config
  (mode satellite)
  (batch-window-ms 500)        ; Aggregate small messages
  (chunk-size-kb 64)           ; Match MTU
  (retry-strategy exponential)
  (max-in-flight 10)           ; Parallelism
  (keepalive-sec 300))         ; 5 min, not 30 sec
</pre>
</section>
<section>
<h2>Synchronization Protocol</h2>
<h3>Lazy Sync (Default)</h3>
<pre>
Node A                              Node B
   |                                   |
   |----(have? [h1, h2, h3])----------&gt;|
   |&lt;---(have [h1, h3] missing [h2])---|
   |----(fetch h2)--------------------&gt;|
   |&lt;---(object h2 ...)----------------|
   |                                   |

No coordination. No locks. No leader.
</pre>
<h3>Crdt-Style Convergence</h3>
<p>Objects are immutable and content-addressed. No conflicts possible at object level.</p>
<p>Manifests (collections of objects) use: - Lamport timestamps for ordering - Last-writer-wins with principal tiebreaker - Or: union (add-only sets)</p>
<pre class="language-scheme">
(manifest
  (name "library")
  (version (lamport 42) (principal "ed25519:..."))
  (entries
    ("rfc-001" "sha512:...")
    ("rfc-002" "sha512:...")
    ...))
</pre>
<h3>Merkle Sync</h3>
<p>Efficient diff for large manifests:</p>
<pre>
         root
        /    \
      h1      h2
     /  \    /  \
    a    b  c    d

Exchange root hash.
If different, recurse on children.
O(log n) round trips to find diff.
</pre>
</section>
<section>
<h2>Federation at Scale</h2>
<h3>Cluster Topology</h3>
<pre>
                    ┌─────────────────────────────────────┐
                    │         COORDINATOR CLUSTER         │
                    │  (3-7 nodes, Byzantine consensus)   │
                    └────────────────┬────────────────────┘
                                     │
          ┌──────────────────────────┼──────────────────────────┐
          │                          │                          │
    ┌─────▼─────┐             ┌──────▼─────┐             ┌──────▼─────┐
    │   FULL    │             │   FULL     │             │    FULL    │
    │  NODES    │             │   NODES    │             │   NODES    │
    └─────┬─────┘             └─────┬──────┘             └─────┬──────┘
          │                         │                          │
    ┌─────▼─────┐             ┌─────▼──────┐             ┌─────▼──────┐
    │ WITNESSES │             │ WITNESSES  │             │ WITNESSES  │
    │ ARCHIVERS │             │ ARCHIVERS  │             │ ARCHIVERS  │
    │   EDGES   │             │   EDGES    │             │   EDGES    │
    └───────────┘             └────────────┘             └────────────┘

Coordinators: Rare, high-capability realms, run consensus
Full: Common realms, replicate everything, serve content
Witnesses: Abundant realms, verify and store, passive sync
Archivers: Cold storage realms, batch sync
Edges: Read-only realms, intermittent, mobile
</pre>
<h3>Partition Tolerance</h3>
<pre>
Network splits → clusters diverge → clusters converge on reconnect

No data loss (content-addressed)
No conflicts (immutable objects)
Audit trails merge (union of entries, Lamport ordering)
Manifests resolve (LWW or union based on type)
</pre>
</section>
<section>
<h2>Git as Export Format</h2>
<h3>The Transition</h3>
<pre>
Phase 1 (Now):      Git is source of truth, vault is cache
Phase 2 (Next):     Vault is source of truth, git is export
Phase 3 (Future):   Git optional, purely for GitHub presence
</pre>
<h3>Export Process</h3>
<pre class="language-scheme">
(git-export
  from: ".vault"
  to: "/tmp/cyberspace-export"
  format: 'git-repo)

;; Creates standard git repo from vault contents
;; For publishing to GitHub, GitLab, etc.
</pre>
<h3>Import Process</h3>
<pre class="language-scheme">
(git-import
  from: "https://github.com/ddp/cyberspace.git"
  to: ".vault")

;; Extracts objects from git, stores in vault
;; Discards git history, keeps content
</pre>
</section>
<section>
<h2>Security at Scale</h2>
<h3>Sybil Resistance</h3>
<p>Problem: Attacker creates many fake nodes to dominate network.</p>
<p>Mitigations: 1. Stake-weighted voting (not proof-of-work, just reputation) 2. Web of trust - new nodes introduced by existing trusted nodes 3. Rate limiting - bound resources per principal 4. Coordinator consensus - Byzantine-resistant core</p>
<h3>Eclipse Attack Resistance</h3>
<p>Problem: Attacker isolates a node by controlling all its peers.</p>
<p>Mitigations: 1. Diverse bootstrap - multiple independent entry points 2. Random peer selection - can't predict who you'll connect to 3. Peer rotation - periodically reconnect to new peers 4. Out-of-band verification - publish peer lists via DNS, blockchain, etc.</p>
<h3>Denial of Service</h3>
<p>Problem: Attacker floods network with junk.</p>
<p>Mitigations: 1. Proof of work on announcements (small, CPU cost) 2. Rate limiting per principal 3. Reputation scoring - misbehaving peers deprioritized 4. Content validation - reject malformed objects immediately</p>
</section>
<section>
<h2>Implementation Phases</h2>
<h3>Phase 1: Native Object Store</h3>
<p>- Implement .vault/objects/ storage - Soup catalog and query - Keep git for development workflow</p>
<h3>Phase 2: Local-First Sync</h3>
<p>- Direct node-to-node protocol - have?/fetch message types - UDP transport with TCP fallback</p>
<h3>Phase 3: Discovery</h3>
<p>- Gossip peer exchange - Bootstrap nodes - Bloom filters for content location</p>
<h3>Phase 4: Scale Testing</h3>
<p>- 100 nodes - 1000 nodes - 10000 nodes - Measure: latency, convergence time, bandwidth</p>
<h3>Phase 5: Git Deprecation</h3>
<p>- Vault as source of truth - Git export for compatibility - Remove git dependency from core operations</p>
</section>
<section>
<h2>Metrics and Monitoring</h2>
<pre class="language-scheme">
(node-metrics)
;; Returns:
((objects-stored 150000)
 (objects-size-gb 50)
 (peers-known 500)
 (peers-connected 20)
 (sync-lag-seconds 30)
 (bandwidth-in-mbps 10)
 (bandwidth-out-mbps 5)
 (requests-per-second 100)
 (errors-per-second 0.1))
</pre>
</section>
<section>
<h2>References</h2>
<p>1. RFC-010: Federation Protocol 2. RFC-016: Lazy Clustering 3. RFC-037: Node Roles and Capabilities 4. Maymounkov, P. (2002). Kademlia: A Peer-to-peer Information System 5. Rabin, M. (1981). Fingerprinting by Random Polynomials 6. IPFS Whitepaper (2014) 7. Shapiro, M. (2011). Conflict-Free Replicated Data Types</p>
</section>
<section>
<h2>Changelog</h2>
<p>- 2026-01-07 - Initial draft</p>
</section>
</body>
</html>
