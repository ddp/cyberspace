%!PS-Adobe-3.0
%%Title: memo-0040-local-inference
%%Creator: Library of Cyberspace Memo Pipeline
%%Pages: (atend)
%%EndComments

/Courier findfont 10 scalefont setfont
/margin 72 def
/pagewidth 612 def
/pageheight 792 def
/leading 12 def
/topmargin pageheight margin sub def
/bottommargin margin def
/linewidth pagewidth margin 2 mul sub def
/ypos topmargin def
/pagenum 1 def

/newline {
  /ypos ypos leading sub def
  ypos bottommargin lt {
    showpage
    /pagenum pagenum 1 add def
    /ypos topmargin def
  } if
  margin ypos moveto
} def

margin topmargin moveto
(Memo 0040: Local Inference Integration) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
() show newline
(------------------------------------------------------------------------) show newline
(ABSTRACT) show newline
(------------------------------------------------------------------------) show newline
() show newline
(This Memo specifies how Library of Cyberspace agents integrate with) show newline
(local large language model \(LLM\) inference backends. Local inference) show newline
(enables privacy-preserving agent operations without external API) show newline
(dependencies, supporting the self-sovereign architecture of the) show newline
(Library.[^h1]) show newline
() show newline
([^h1]: Historical: The tension between local and remote computation) show newline
(echoes the mainframe-to-PC transition. Local inference returns agency to) show newline
(the edge, reversing decades of cloud centralization.) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(MOTIVATION) show newline
(------------------------------------------------------------------------) show newline
() show newline
(Agents in the Library of Cyberspace require language understanding and) show newline
(generation capabilities for:) show newline
() show newline
(  * Document summarization and indexing) show newline
(  * Natural language query translation \(Memo-025\)) show newline
(  * Content annotation and metadata extraction) show newline
(  * Inter-agent communication in natural language) show newline
() show newline
(These tasks require linguistic intelligence that rules-based systems) show newline
(cannot provide at scale.) show newline
() show newline
(External API dependencies \(OpenAI, Anthropic, etc.\) introduce:) show newline
() show newline
(  * Privacy leakage - document content leaves the realm) show newline
(  * Availability risk - network partitions break agent operation) show newline
(  * Cost unpredictability - metered APIs scale poorly) show newline
(  * Vendor lock-in - proprietary formats and rate limits) show newline
() show newline
(Each dependency on external APIs undermines the self-sovereign) show newline
(architecture; intelligence that phones home is not truly yours.) show newline
() show newline
(Local inference eliminates these concerns while maintaining) show newline
(capability.[^d1]) show newline
() show newline
([^d1]: Design: We deliberately avoid specifying model architectures. The) show newline
(interface is model-agnostic—agents negotiate capabilities at runtime.) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(ARCHITECTURE) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Inference Topology) show newline
(------------------) show newline
() show newline
(Table 1: Inference Deployment Models) show newline
() show newline
() show newline
(  Model         Description                     Use Case                      ) show newline
(  Realm-local   Inference server within realm   Default, privacy-preserving   ) show newline
(  Node-local    Per-node inference              Edge agents, mobile           ) show newline
(  Federated     Shared across trusted realms    Resource pooling              ) show newline
() show newline
() show newline
(    +-----------------------------------------------------+) show newline
(    |                      Realm                          |) show newline
(    |  +---------+    +--------------+    +-----------+  |) show newline
(    |  |  Agent  |--->|   Inference  |--->|   Model   |  |) show newline
(    |  |         |<---|    Server    |<---|  Weights  |  |) show newline
(    |  +---------+    +--------------+    +-----------+  |) show newline
(    |       |              :11434              |          |) show newline
(    |       v                                  v          |) show newline
(    |  +---------+                      +-----------+    |) show newline
(    |  |  Vault  |                      |   VRAM    |    |) show newline
(    |  +---------+                      +-----------+    |) show newline
(    +-----------------------------------------------------+) show newline
() show newline
() show newline
(Protocol) show newline
(--------) show newline
() show newline
(Agents communicate with inference servers via HTTP REST API.[^i1]) show newline
() show newline
([^i1]: Implementation: Ollama exposes an OpenAI-compatible API at) show newline
(/v1/chat/completions and a native API at /api/generate. We specify both) show newline
(for maximum compatibility.) show newline
() show newline
(Discovery:) show newline
() show newline
() show newline
(    \(define \(discover-inference-server\)) show newline
(      ;; Check well-known locations) show newline
(      \(or \(probe "http://localhost:11434"\)      ; Ollama default) show newline
(          \(probe "http://localhost:8080"\)       ; llama.cpp) show newline
(          \(probe \(realm-config 'inference-url\)\) ; Configured) show newline
(          #f\)\)                                   ; None available) show newline
() show newline
(Capability Negotiation:) show newline
() show newline
() show newline
(    \(define \(negotiate-capabilities server\)) show newline
(      \(let \(\(models \(http-get \(string-append server "/api/tags"\)\)\)\)) show newline
(        \(map \(lambda \(m\)) show newline
(               `\(\(name . ,\(alist-ref 'name m\)\)) show newline
(                 \(parameters . ,\(alist-ref 'size m\)\)) show newline
(                 \(context-length . ,\(model-context-length m\)\)) show newline
(                 \(capabilities . ,\(model-capabilities m\)\)\)\)) show newline
(             models\)\)\)) show newline
() show newline
(Table 2: Model Capabilities) show newline
() show newline
() show newline
(  Capability   Description               Example Models            ) show newline
(  completion   Text generation           All                       ) show newline
(  chat         Multi-turn conversation   Llama 3, Mistral          ) show newline
(  embedding    Vector embeddings         nomic-embed, mxbai        ) show newline
(  code         Code generation           CodeLlama, DeepSeek       ) show newline
(  vision       Image understanding       LLaVA, Llama 3.2 Vision   ) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(AGENT INTEGRATION) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Scribe Agent) show newline
(------------) show newline
() show newline
(The scribe agent uses local inference for document processing:[^r1]) show newline
() show newline
([^r1]: Research: Retrieval-augmented generation \(RAG\) combines local) show newline
(inference with vault content retrieval. See Lewis et al., "Retrieval-) show newline
(Augmented Generation for Knowledge-Intensive NLP Tasks" \(2020\).) show newline
() show newline
() show newline
(    \(define \(scribe-summarize document\)) show newline
(      \(let \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'completion\)\)) show newline
(             \(prompt \(format "Summarize the following document:\\n\\n~a") show newline
(                             \(document-content document\)\)\)\)) show newline
(        \(inference-complete server model prompt) show newline
(                            '\(\(max-tokens . 500\)) show newline
(                              \(temperature . 0.3\)\)\)\)\)) show newline
(    \(define \(scribe-index document\)) show newline
(      ;; Extract keywords using local inference) show newline
(      \(let \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'completion\)\)) show newline
(             \(prompt \(format "Extract 5-10 keywords from:\\n\\n~a") show newline
(                             \(document-content document\)\)\)\)) show newline
(        \(parse-keywords \(inference-complete server model prompt\)\)\)\)) show newline
() show newline
() show newline
(Query Translation) show newline
(-----------------) show newline
() show newline
(Natural language queries translate to Memo-025 query language:) show newline
() show newline
() show newline
(    \(define \(nl-to-query natural-language\)) show newline
(      \(let* \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'chat\)\)) show newline
(             \(system "You translate natural language to Cyberspace query) show newline
(                      language. Output only the query, no explanation."\)) show newline
(             \(examples '\(\("find all RFCs about security") show newline
(                          "\(query \(type rfc\) \(topic security\)\)"\)) show newline
(                         \("documents modified last week") show newline
(                          "\(query \(modified \(after \(days-ago 7\)\)\)\)"\)\)\)\)) show newline
(        \(inference-chat server model system examples natural-language\)\)\)) show newline
() show newline
() show newline
(Demonic Agent Inference) show newline
(-----------------------) show newline
() show newline
(Sandboxed agents \(Memo-023\) access inference through capability) show newline
(tokens:[^d2]) show newline
() show newline
([^d2]: Design: Inference capability is granted like any other—via) show newline
(Simple Public Key Infrastructure \(SPKI\) certificate. An agent cannot) show newline
(infer without explicit authorization.) show newline
() show newline
() show newline
(    \(define \(demonic-inference agent prompt\)) show newline
(      \(let \(\(cap \(agent-capability agent 'inference\)\)\)) show newline
(        \(if \(not cap\)) show newline
(            \(error 'unauthorized "Agent lacks inference capability"\)) show newline
(            \(let \(\(limits \(capability-limits cap\)\)\)) show newline
(              \(enforce-limits limits\)) show newline
(              \(inference-complete \(capability-server cap\)) show newline
(                                  \(capability-model cap\)) show newline
(                                  prompt) show newline
(                                  limits\)\)\)\)\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(RESOURCE MANAGEMENT) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(VRAM Allocation) show newline
(---------------) show newline
() show newline
(Table 3: Model Size Guidelines) show newline
() show newline
() show newline
(  Parameters   VRAM \(Q4\)   VRAM \(FP16\)   Context   ) show newline
(  7B           4 GB        14 GB         8K        ) show newline
(  13B          8 GB        26 GB         8K        ) show newline
(  34B          20 GB       68 GB         16K       ) show newline
(  70B          40 GB       140 GB        32K       ) show newline
() show newline
() show newline
(Rate Limiting) show newline
(-------------) show newline
() show newline
(Local inference is limited by hardware, not API quotas. Realms implement) show newline
(fair scheduling across agents:[^i2]) show newline
() show newline
([^i2]: Implementation: Token bucket algorithm with per-agent quotas.) show newline
(Prevents single agent from monopolizing inference resources.) show newline
() show newline
() show newline
(    \(define \(inference-rate-limit agent tokens\)) show newline
(      \(let \(\(bucket \(agent-token-bucket agent\)\)\)) show newline
(        \(if \(bucket-consume bucket tokens\)) show newline
(            #t) show newline
(            \(begin) show newline
(              \(agent-wait agent \(bucket-refill-time bucket\)\)) show newline
(              \(inference-rate-limit agent tokens\)\)\)\)\)) show newline
() show newline
() show newline
(Fallback Behavior) show newline
(-----------------) show newline
() show newline
(When local inference is unavailable:) show newline
() show newline
(  * Queue - buffer requests for later processing) show newline
(  * Degrade - use simpler heuristics \(keyword extraction vs. LLM\)) show newline
(  * Federate - request inference from trusted peer realm) show newline
(  * Fail - return error to requesting agent) show newline
() show newline
(Graceful degradation ensures agents remain functional even when full) show newline
(inference capacity is unavailable.) show newline
() show newline
() show newline
(    \(define \(inference-with-fallback server model prompt\)) show newline
(      \(or \(try-inference server model prompt\)) show newline
(          \(try-queued-inference prompt\)) show newline
(          \(try-degraded-processing prompt\)) show newline
(          \(try-federated-inference prompt\)) show newline
(          \(error 'inference-unavailable\)\)\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(SECURITY CONSIDERATIONS) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Prompt Injection) show newline
(----------------) show newline
() show newline
(Agents MUST sanitize document content before inclusion in prompts:[^r2]) show newline
() show newline
([^r2]: Research: Prompt injection attacks embed malicious instructions) show newline
(in user content. See Perez & Ribeiro, "Ignore This Title and) show newline
(HackAPrompt" \(2023\).) show newline
() show newline
() show newline
(    \(define \(safe-prompt template document\)) show newline
(      \(let \(\(sanitized \(sanitize-for-prompt \(document-content document\)\)\)\)) show newline
(        \(format template sanitized\)\)\)) show newline
(    \(define \(sanitize-for-prompt text\)) show newline
(      ;; Remove instruction-like patterns) show newline
(      \(regexp-replace-all) show newline
(        '\("ignore previous" "disregard" "new instructions" "system:"\)) show newline
(        text) show newline
(        "[REDACTED]"\)\)) show newline
() show newline
() show newline
(Model Provenance) show newline
(----------------) show newline
() show newline
(Realms SHOULD verify model checksums before loading:[^d3]) show newline
() show newline
([^d3]: Design: Model weights can contain backdoors. Verification against) show newline
(known-good checksums prevents supply chain attacks.) show newline
() show newline
() show newline
(    \(define \(verify-model model-path expected-hash\)) show newline
(      \(let \(\(actual-hash \(sha256-file model-path\)\)\)) show newline
(        \(unless \(equal? actual-hash expected-hash\)) show newline
(          \(error 'model-verification-failed) show newline
(                 "Model checksum mismatch"\)\)\)\)) show newline
() show newline
() show newline
(Inference Isolation) show newline
(-------------------) show newline
() show newline
(Sensitive documents require isolated inference contexts:) show newline
() show newline
(  * Separate model instances per security domain) show newline
(  * Clear KV cache between requests from different agents) show newline
(  * No persistent memory across security boundaries) show newline
() show newline
(LLMs have memory; cross-agent inference without isolation is cross-agent) show newline
(information leakage.) show newline
() show newline
(------------------------------------------------------------------------) show newline

showpage
%%Trailer
%%Pages: pagenum
%%EOF
