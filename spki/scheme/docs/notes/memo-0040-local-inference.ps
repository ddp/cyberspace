%!PS-Adobe-3.0
%%Title: memo-0040-local-inference
%%Creator: Library of Cyberspace Memo Pipeline
%%Pages: (atend)
%%EndComments

/Courier findfont 10 scalefont setfont
/margin 72 def
/pagewidth 612 def
/pageheight 792 def
/leading 12 def
/topmargin pageheight margin sub def
/bottommargin margin def
/linewidth pagewidth margin 2 mul sub def
/ypos topmargin def
/pagenum 1 def

/newline {
  /ypos ypos leading sub def
  ypos bottommargin lt {
    showpage
    /pagenum pagenum 1 add def
    /ypos topmargin def
  } if
  margin ypos moveto
} def

margin topmargin moveto
(Memo 0040: Local Inference Integration) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
() show newline
(------------------------------------------------------------------------) show newline
(ABSTRACT) show newline
(------------------------------------------------------------------------) show newline
() show newline
(This Memo specifies how Library of Cyberspace agents integrate with) show newline
(local large language model \(LLM\) inference backends. Local inference) show newline
(enables privacy-preserving agent operations without external API) show newline
(dependencies, supporting the self-sovereign architecture of the) show newline
(Library.[^h1]) show newline
() show newline
([^h1]: Historical: The tension between local and remote computation) show newline
(echoes the mainframe-to-PC transition. Local inference returns agency to) show newline
(the edge, reversing decades of cloud centralization.) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(MOTIVATION) show newline
(------------------------------------------------------------------------) show newline
() show newline
(Agents in the Library of Cyberspace require language understanding and) show newline
(generation capabilities for:) show newline
() show newline
(  * Document summarization and indexing) show newline
(  * Natural language query translation \(Memo-025\)) show newline
(  * Content annotation and metadata extraction) show newline
(  * Inter-agent communication in natural language) show newline
() show newline
(External API dependencies \(OpenAI, Anthropic, etc.\) introduce:) show newline
() show newline
(1. Privacy leakage — document content leaves the realm 2. Availability) show newline
(risk — network partitions break agent operation 3. Cost) show newline
(unpredictability — metered APIs scale poorly 4. Vendor lock-in —) show newline
(proprietary formats and rate limits) show newline
() show newline
(Local inference eliminates these concerns while maintaining) show newline
(capability.[^d1]) show newline
() show newline
([^d1]: Design: We deliberately avoid specifying model architectures. The) show newline
(interface is model-agnostic—agents negotiate capabilities at runtime.) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(ARCHITECTURE) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Inference Topology) show newline
(------------------) show newline
() show newline
(Table 1: Inference Deployment Models) show newline
() show newline
() show newline
(  Model         Description                     Use Case                      ) show newline
(  Realm-local   Inference server within realm   Default, privacy-preserving   ) show newline
(  Node-local    Per-node inference              Edge agents, mobile           ) show newline
(  Federated     Shared across trusted realms    Resource pooling              ) show newline
() show newline
() show newline
(    +-----------------------------------------------------+) show newline
(    |                      Realm                          |) show newline
(    |  +---------+    +--------------+    +-----------+  |) show newline
(    |  |  Agent  |--->|   Inference  |--->|   Model   |  |) show newline
(    |  |         |<---|    Server    |<---|  Weights  |  |) show newline
(    |  +---------+    +--------------+    +-----------+  |) show newline
(    |       |              :11434              |          |) show newline
(    |       v                                  v          |) show newline
(    |  +---------+                      +-----------+    |) show newline
(    |  |  Vault  |                      |   VRAM    |    |) show newline
(    |  +---------+                      +-----------+    |) show newline
(    +-----------------------------------------------------+) show newline
() show newline
() show newline
(Protocol) show newline
(--------) show newline
() show newline
(Agents communicate with inference servers via HTTP REST API.[^i1]) show newline
() show newline
([^i1]: Implementation: Ollama exposes an OpenAI-compatible API at) show newline
(/v1/chat/completions and a native API at /api/generate. We specify both) show newline
(for maximum compatibility.) show newline
() show newline
(Discovery:) show newline
() show newline
() show newline
(    \(define \(discover-inference-server\)) show newline
(      ;; Check well-known locations) show newline
(      \(or \(probe "http://localhost:11434"\)      ; Ollama default) show newline
(          \(probe "http://localhost:8080"\)       ; llama.cpp) show newline
(          \(probe \(realm-config 'inference-url\)\) ; Configured) show newline
(          #f\)\)                                   ; None available) show newline
() show newline
(Capability Negotiation:) show newline
() show newline
() show newline
(    \(define \(negotiate-capabilities server\)) show newline
(      \(let \(\(models \(http-get \(string-append server "/api/tags"\)\)\)\)) show newline
(        \(map \(lambda \(m\)) show newline
(               `\(\(name . ,\(alist-ref 'name m\)\)) show newline
(                 \(parameters . ,\(alist-ref 'size m\)\)) show newline
(                 \(context-length . ,\(model-context-length m\)\)) show newline
(                 \(capabilities . ,\(model-capabilities m\)\)\)\)) show newline
(             models\)\)\)) show newline
() show newline
(Table 2: Model Capabilities) show newline
() show newline
() show newline
(  Capability   Description               Example Models            ) show newline
(  completion   Text generation           All                       ) show newline
(  chat         Multi-turn conversation   Llama 3, Mistral          ) show newline
(  embedding    Vector embeddings         nomic-embed, mxbai        ) show newline
(  code         Code generation           CodeLlama, DeepSeek       ) show newline
(  vision       Image understanding       LLaVA, Llama 3.2 Vision   ) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(AGENT INTEGRATION) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Scribe Agent) show newline
(------------) show newline
() show newline
(The scribe agent uses local inference for document processing:[^r1]) show newline
() show newline
([^r1]: Research: Retrieval-augmented generation \(RAG\) combines local) show newline
(inference with vault content retrieval. See Lewis et al., "Retrieval-) show newline
(Augmented Generation for Knowledge-Intensive NLP Tasks" \(2020\).) show newline
() show newline
() show newline
(    \(define \(scribe-summarize document\)) show newline
(      \(let \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'completion\)\)) show newline
(             \(prompt \(format "Summarize the following document:\\n\\n~a") show newline
(                             \(document-content document\)\)\)\)) show newline
(        \(inference-complete server model prompt) show newline
(                            '\(\(max-tokens . 500\)) show newline
(                              \(temperature . 0.3\)\)\)\)\)) show newline
(    \(define \(scribe-index document\)) show newline
(      ;; Extract keywords using local inference) show newline
(      \(let \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'completion\)\)) show newline
(             \(prompt \(format "Extract 5-10 keywords from:\\n\\n~a") show newline
(                             \(document-content document\)\)\)\)) show newline
(        \(parse-keywords \(inference-complete server model prompt\)\)\)\)) show newline
() show newline
() show newline
(Query Translation) show newline
(-----------------) show newline
() show newline
(Natural language queries translate to Memo-025 query language:) show newline
() show newline
() show newline
(    \(define \(nl-to-query natural-language\)) show newline
(      \(let* \(\(server \(discover-inference-server\)\)) show newline
(             \(model \(select-model server 'chat\)\)) show newline
(             \(system "You translate natural language to Cyberspace query) show newline
(                      language. Output only the query, no explanation."\)) show newline
(             \(examples '\(\("find all RFCs about security") show newline
(                          "\(query \(type rfc\) \(topic security\)\)"\)) show newline
(                         \("documents modified last week") show newline
(                          "\(query \(modified \(after \(days-ago 7\)\)\)\)"\)\)\)\)) show newline
(        \(inference-chat server model system examples natural-language\)\)\)) show newline
() show newline
() show newline
(Demonic Agent Inference) show newline
(-----------------------) show newline
() show newline
(Sandboxed agents \(Memo-023\) access inference through capability) show newline
(tokens:[^d2]) show newline
() show newline
([^d2]: Design: Inference capability is granted like any other—via SPKI) show newline
(certificate. An agent cannot infer without explicit authorization.) show newline
() show newline
() show newline
(    \(define \(demonic-inference agent prompt\)) show newline
(      \(let \(\(cap \(agent-capability agent 'inference\)\)\)) show newline
(        \(if \(not cap\)) show newline
(            \(error 'unauthorized "Agent lacks inference capability"\)) show newline
(            \(let \(\(limits \(capability-limits cap\)\)\)) show newline
(              \(enforce-limits limits\)) show newline
(              \(inference-complete \(capability-server cap\)) show newline
(                                  \(capability-model cap\)) show newline
(                                  prompt) show newline
(                                  limits\)\)\)\)\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(RESOURCE MANAGEMENT) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(VRAM Allocation) show newline
(---------------) show newline
() show newline
(Table 3: Model Size Guidelines) show newline
() show newline
() show newline
(  Parameters   VRAM \(Q4\)   VRAM \(FP16\)   Context   ) show newline
(  7B           4 GB        14 GB         8K        ) show newline
(  13B          8 GB        26 GB         8K        ) show newline
(  34B          20 GB       68 GB         16K       ) show newline
(  70B          40 GB       140 GB        32K       ) show newline
() show newline
() show newline
(Rate Limiting) show newline
(-------------) show newline
() show newline
(Local inference is limited by hardware, not API quotas. Realms implement) show newline
(fair scheduling across agents:[^i2]) show newline
() show newline
([^i2]: Implementation: Token bucket algorithm with per-agent quotas.) show newline
(Prevents single agent from monopolizing inference resources.) show newline
() show newline
() show newline
(    \(define \(inference-rate-limit agent tokens\)) show newline
(      \(let \(\(bucket \(agent-token-bucket agent\)\)\)) show newline
(        \(if \(bucket-consume bucket tokens\)) show newline
(            #t) show newline
(            \(begin) show newline
(              \(agent-wait agent \(bucket-refill-time bucket\)\)) show newline
(              \(inference-rate-limit agent tokens\)\)\)\)\)) show newline
() show newline
() show newline
(Fallback Behavior) show newline
(-----------------) show newline
() show newline
(When local inference is unavailable:) show newline
() show newline
(1. Queue — buffer requests for later processing 2. Degrade — use) show newline
(simpler heuristics \(keyword extraction vs. LLM\) 3. Federate — request) show newline
(inference from trusted peer realm 4. Fail — return error to requesting) show newline
(agent) show newline
() show newline
() show newline
(    \(define \(inference-with-fallback server model prompt\)) show newline
(      \(or \(try-inference server model prompt\)) show newline
(          \(try-queued-inference prompt\)) show newline
(          \(try-degraded-processing prompt\)) show newline
(          \(try-federated-inference prompt\)) show newline
(          \(error 'inference-unavailable\)\)\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(SECURITY CONSIDERATIONS) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Prompt Injection) show newline
(----------------) show newline
() show newline
(Agents MUST sanitize document content before inclusion in prompts:[^r2]) show newline
() show newline
([^r2]: Research: Prompt injection attacks embed malicious instructions) show newline
(in user content. See Perez & Ribeiro, "Ignore This Title and) show newline
(HackAPrompt" \(2023\).) show newline
() show newline
() show newline
(    \(define \(safe-prompt template document\)) show newline
(      \(let \(\(sanitized \(sanitize-for-prompt \(document-content document\)\)\)\)) show newline
(        \(format template sanitized\)\)\)) show newline
(    \(define \(sanitize-for-prompt text\)) show newline
(      ;; Remove instruction-like patterns) show newline
(      \(regexp-replace-all) show newline
(        '\("ignore previous" "disregard" "new instructions" "system:"\)) show newline
(        text) show newline
(        "[REDACTED]"\)\)) show newline
() show newline
() show newline
(Model Provenance) show newline
(----------------) show newline
() show newline
(Realms SHOULD verify model checksums before loading:[^d3]) show newline
() show newline
([^d3]: Design: Model weights can contain backdoors. Verification against) show newline
(known-good checksums prevents supply chain attacks.) show newline
() show newline
() show newline
(    \(define \(verify-model model-path expected-hash\)) show newline
(      \(let \(\(actual-hash \(sha256-file model-path\)\)\)) show newline
(        \(unless \(equal? actual-hash expected-hash\)) show newline
(          \(error 'model-verification-failed) show newline
(                 "Model checksum mismatch"\)\)\)\)) show newline
() show newline
() show newline
(Inference Isolation) show newline
(-------------------) show newline
() show newline
(Sensitive documents require isolated inference contexts:) show newline
() show newline
(  * Separate model instances per security domain) show newline
(  * Clear KV cache between requests from different agents) show newline
(  * No persistent memory across security boundaries) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(OLLAMA INTEGRATION) show newline
(------------------------------------------------------------------------) show newline
() show newline
(Ollama is the reference implementation for local inference.[^h2]) show newline
() show newline
([^h2]: Historical: Ollama follows the Unix philosophy—do one thing) show newline
(well. It manages models and serves inference. Nothing more.) show newline
() show newline
() show newline
(Installation) show newline
(------------) show newline
() show newline
() show newline
(    # macOS / Linux) show newline
(    curl -fsSL https://ollama.com/install.sh | sh) show newline
(    # Verify) show newline
(    ollama --version) show newline
() show newline
() show newline
(Model Management) show newline
(----------------) show newline
() show newline
() show newline
(    # Pull models) show newline
(    ollama pull llama3) show newline
(    ollama pull nomic-embed-text) show newline
(    # List available) show newline
(    ollama list) show newline
(    # Remove) show newline
(    ollama rm unused-model) show newline
() show newline
() show newline
(API Endpoints) show newline
(-------------) show newline
() show newline
(Table 4: Ollama API Endpoints) show newline
() show newline
() show newline
(  Endpoint          Method   Purpose             ) show newline
(  /api/generate     POST     Text completion     ) show newline
(  /api/chat         POST     Multi-turn chat     ) show newline
(  /api/embeddings   POST     Vector embeddings   ) show newline
(  /api/tags         GET      List models         ) show newline
(  /api/show         POST     Model details       ) show newline
() show newline
() show newline
(Scheme Bindings) show newline
(---------------) show newline
() show newline
() show newline
(    \(define ollama-base "http://localhost:11434"\)) show newline
(    \(define \(ollama-generate model prompt #!key \(options '\(\)\)\)) show newline
(      \(http-post \(string-append ollama-base "/api/generate"\)) show newline
(                 \(\(model . ,model\)) show newline
(                   \(prompt . ,prompt\)) show newline
(                   \(stream . #f\)) show newline
(                   ,@options\)\)\)) show newline
(    \(define \(ollama-chat model messages #!key \(options '\(\)\)\)) show newline
(      \(http-post \(string-append ollama-base "/api/chat"\)) show newline
(                 \(\(model . ,model\)) show newline
(                   \(messages . ,messages\)) show newline
(                   \(stream . #f\)) show newline
(                   ,@options\)\)\)) show newline
(    \(define \(ollama-embed model text\)) show newline
(      \(http-post \(string-append ollama-base "/api/embeddings"\)) show newline
(                 `\(\(model . ,model\)) show newline
(                   \(prompt . ,text\)\)\)\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(COMPATIBILITY) show newline
(------------------------------------------------------------------------) show newline
() show newline
() show newline
(Alternative Backends) show newline
(--------------------) show newline
() show newline
(The protocol supports any OpenAI-compatible inference server:) show newline
() show newline
(  * llama.cpp — server binary with --host flag - vLLM — production serving with PagedAttention) show newline
(  * LocalAI — drop-in OpenAI replacement) show newline
(  * LM Studio — GUI with server mode) show newline
() show newline
() show newline
(Cloud Fallback) show newline
(--------------) show newline
() show newline
(For realms without local GPU resources, cloud inference MAY be used as) show newline
(fallback with explicit user consent and encryption:) show newline
() show newline
() show newline
(    \(define \(cloud-inference-fallback prompt\)) show newline
(      \(when \(user-consent? 'cloud-inference\)) show newline
(        \(let \(\(encrypted \(encrypt-for-cloud prompt\)\)\)) show newline
(          ;; Use cloud API with encrypted prompt) show newline
(          ;; Decrypt response locally) show newline
(          \)\)\)) show newline
() show newline
(This is NOT RECOMMENDED for sensitive documents.) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(REFERENCES) show newline
(------------------------------------------------------------------------) show newline
() show newline
(1. Memo-023: Demonic Agent Sandboxing 2. Memo-025: Query Language 3.) show newline
(Memo-035: Mobile Agents and Pub/Sub 4. Ollama Documentation:) show newline
(https://ollama.com/ 5. Lewis et al., "Retrieval-Augmented Generation") show newline
(\(2020\)) show newline
() show newline
() show newline
(------------------------------------------------------------------------) show newline
(CHANGELOG) show newline
(------------------------------------------------------------------------) show newline
() show newline
(  * 2026-01-07) show newline
(  * Initial specification) show newline
() show newline
(------------------------------------------------------------------------) show newline

showpage
%%Trailer
%%Pages: pagenum
%%EOF
