Memo 0039: Realm Roles and Capabilities


------------------------------------------------------------------------

------------------------------------------------------------------------
ABSTRACT
------------------------------------------------------------------------

This Memo defines functional roles for nodes in a Library of Cyberspace
confederation based on compute, storage, network, and security
capabilities. Roles determine what operations a node can perform and how
it participates in the distributed system.


------------------------------------------------------------------------
TERMINOLOGY
------------------------------------------------------------------------

Realm: A node's place in cyberspace. A realm encompasses: - The node's
vault (local content-addressed object store) - The node's principal
(Ed25519 identity) - The node's capabilities (hardware, network,
security) - The node's objects (what it stores and serves)

A realm is local-first and sovereign. The node controls what to share,
who to trust, what to replicate. When nodes federate, their realms
overlap - objects flow between them according to trust relationships.

The hardware manifest stored at .vault/node-hardware declares what kind
of place this realm occupies in cyberspace.


------------------------------------------------------------------------
MOTIVATION
------------------------------------------------------------------------

Memo-010 (Federation Protocol) defines trust relationships between peers
(publisher, subscriber, peer). However, it does not address functional
capabilities - what operations each node can actually perform based on
its hardware and network constraints.

A Raspberry Pi on a solar-powered satellite uplink has different
capabilities than a rack-mounted server in a datacenter. The system
should:

  * Self-assess - Nodes should know their own capabilities
  * Declare - Nodes should advertise their role to peers
  * Adapt - Operations should degrade gracefully based on available roles
  * Persist - Role assignments should survive restarts

Automatic role detection enables heterogeneous hardware to participate
appropriately without manual configuration.

Without explicit role management, the system either assumes all nodes
are equal or requires manual configuration; neither scales.


------------------------------------------------------------------------
NODE ROLES
------------------------------------------------------------------------


Role Hierarchy
--------------


                        ┌─────────────┐
                        │ COORDINATOR │  Byzantine consensus, threshold signing
                        │   (rare)    │  Always-on, high compute
                        └──────┬──────┘
                               │
                  ┌────────────┼────────────┐
                  ▼            ▼            ▼
           ┌───────────┐ ┌───────────┐ ┌───────────┐
           │   FULL    │ │  WITNESS  │ │ ARCHIVER  │
           │   NODE    │ │           │ │           │
           └─────┬─────┘ └───────────┘ └───────────┘
                 │
                 ▼
           ┌───────────┐
           │   EDGE    │  Read-only, mobile, intermittent
           └───────────┘


Role Definitions
----------------


  Role          Compute   Storage   Network        Operations                                             
  coordinator   High      Medium    Always-on      Byzantine consensus, threshold signing, key ceremony   
  full          Medium    High      Reliable       All vault operations, replication origin               
  witness       Low       High      Intermittent   Passive storage, hash verification, audit              
  archiver      Low       Maximum   Batch          Cold storage, offline preservation                     
  edge          Minimal   Minimal   Sporadic       Read-only sync, mobile access                          


Capability Requirements
-----------------------


    (node-role-capabilities
      (coordinator
        (compute    (min-cores 4) (min-ram-gb 8))
        (storage    (min-gb 100) (type ssd))
        (network    (uptime 0.99) (latency-ms 50))
        (security   (hsm optional) (secure-enclave optional)))
      (full
        (compute    (min-cores 2) (min-ram-gb 4))
        (storage    (min-gb 500) (type any))
        (network    (uptime 0.95) (latency-ms 200))
        (security   (signing-key required)))
      (witness
        (compute    (min-cores 1) (min-ram-gb 1))
        (storage    (min-gb 100) (type any))
        (network    (uptime 0.50) (latency-ms 1000))
        (security   (verify-key required)))
      (archiver
        (compute    (min-cores 1) (min-ram-gb 512mb))
        (storage    (min-gb 1000) (type cold))
        (network    (uptime 0.10) (batch-ok #t))
        (security   (verify-key required) (offline-ok #t)))
      (edge
        (compute    (min-cores 1) (min-ram-gb 256mb))
        (storage    (min-gb 1) (type any))
        (network    (uptime 0.01) (latency-ms 5000))
        (security   (read-only #t))))


------------------------------------------------------------------------
ROLE DETECTION
------------------------------------------------------------------------


Automatic Probing
-----------------


    (define (node-probe-capabilities)
      "Probe local system capabilities"
      `((compute
         (cores ,(get-cpu-cores))
         (ram-gb ,(get-ram-gb))
         (load-avg ,(get-load-average)))
        (storage
         (available-gb ,(get-available-storage))
         (type ,(detect-storage-type)))
        (network
         (latency-ms ,(probe-network-latency))
         (bandwidth-mbps ,(estimate-bandwidth))
         (type ,(detect-network-type)))  ; ethernet, wifi, cellular, satellite
        (security
         (signing-key ,(has-signing-key?))
         (verify-key ,(has-verify-key?))
         (hsm ,(has-hsm?)))))


Role Assignment
---------------


    (define (node-assign-role capabilities)
      "Assign role based on probed capabilities"
      (let ((compute (assq 'compute capabilities))
            (storage (assq 'storage capabilities))
            (network (assq 'network capabilities)))
        (cond
         ;; Coordinator: high everything
         ((and (>= (get-cores compute) 4)
               (>= (get-ram compute) 8)
               (<= (get-latency network) 50))
          'coordinator)
         ;; Full node: medium compute, high storage
         ((and (>= (get-cores compute) 2)
               (>= (get-storage storage) 500))
          'full)
         ;; Archiver: low compute, massive storage, batch network
         ((and (>= (get-storage storage) 1000)
               (eq? (get-network-type network) 'batch))
          'archiver)
         ;; Witness: low compute, decent storage
         ((>= (get-storage storage) 100)
          'witness)
         ;; Edge: everything else
         (else 'edge))))


------------------------------------------------------------------------
ROLE DECLARATION
------------------------------------------------------------------------


Local Configuration
-------------------


    ;; ~/.cyberspace/node-role
    (node-config
      (role witness)                    ; Declared role
      (auto-detect #f)                  ; Don't override with probed
      (capabilities                     ; Known constraints
        (network (type satellite)
                 (latency-ms 600)
                 (bandwidth-mbps 100))))


Role Announcement
-----------------


    (define (node-announce-role)
      "Announce role to federation peers"
      (let ((role (node-current-role))
            (caps (node-probe-capabilities))
            (key (vault-config 'signing-key)))
        (when key
          (let ((announcement
                 `(node-role-announcement
                   (principal ,(get-vault-principal key))
                   (role ,role)
                   (capabilities ,caps)
                   (timestamp ,(current-seconds)))))
            ;; Sign and broadcast
            (federation-broadcast
             (sign-announcement announcement key))))))


------------------------------------------------------------------------
ROLE-BASED OPERATION CONSTRAINTS
------------------------------------------------------------------------


Operation Matrix
----------------


  Operation          coordinator   full   witness   archiver   edge   
  seal-commit        
  seal-release       
  seal-archive       
  seal-restore       
  seal-publish       
  seal-subscribe     
  seal-synchronize   
  seal-verify        
  threshold-sign     
  byzantine-vote     
  key-ceremony       
  audit-append       
  audit-verify       


Graceful Degradation
--------------------


    (define (node-can-perform? operation)
      "Check if current role permits operation"
      (let ((role (node-current-role))
            (required (operation-required-role operation)))
        (role-permits? role required)))
    (define (role-permits? actual required)
      "Check role hierarchy"
      (let ((hierarchy '(coordinator full witness archiver edge)))
        (<= (list-index hierarchy actual)
            (list-index hierarchy required))))


------------------------------------------------------------------------
STARLINK CONSIDERATIONS
------------------------------------------------------------------------

Per Memo-016, the system is optimized for satellite links:


    (node-config
      (role witness)
      (network
        (type satellite)
        (provider starlink)
        (characteristics
          (latency-ms 20-40)           ; Low-earth orbit
          (bandwidth-mbps 100-200)     ; Bursty
          (jitter high)                ; Variable
          (uptime 0.95)                ; Weather dependent
          (data-cap none))))           ; Unlimited for now
    ;; Satellite-optimized behavior
    (define satellite-mode
      '((batch-sync #t)                ; Aggregate operations
        (lazy-pull #t)                 ; Don't fetch eagerly
        (compress-always #t)           ; Minimize transfer
        (retry-aggressive #t)          ; Handle drops
        (heartbeat-interval 300)))     ; 5 min, not seconds


Role Implications for Satellite Nodes
-------------------------------------

- coordinator: Generally NOT suitable for satellite (latency too
variable for consensus) - full: Marginal (can work with lazy clustering)
- witness: IDEAL (passive, tolerates latency) - archiver: IDEAL (batch
operations) - edge: IDEAL (intermittent by design)


------------------------------------------------------------------------
IMPLEMENTATION
------------------------------------------------------------------------


REPL Commands
-------------


    ;; Probe and display capabilities
    (node-probe)
    ;; Show current role
    (node-role)
    ;; Set role explicitly
    (node-role 'witness)
    ;; Check if operation permitted
    (node-can? 'threshold-sign)
    ;; Announce role to peers
    (node-announce)


Persistence
-----------

Role configuration stored in the realm:


    ~/.cyberspace/node-role      ; User override (global)
    .vault/node-role             ; Realm-specific role
    .vault/node-hardware         ; Hardware manifest (auto-refreshed)

The hardware manifest is automatically updated on REPL startup,
declaring the realm's capabilities to federated peers.


Audit Trail
-----------

Role changes are auditable events:


    (audit-entry
      (type node-role-change)
      (timestamp 1736280000)
      (from edge)
      (to witness)
      (reason "Storage expanded")
      (actor #${principal}))


------------------------------------------------------------------------
SECURITY CONSIDERATIONS
------------------------------------------------------------------------


Role Spoofing
-------------

A node could claim a higher role than its capabilities warrant.
Mitigations:

  * Capability proofs: Require benchmark results
  * Peer validation: Other nodes can challenge claims
  * Reputation: Track role fulfillment history
  * Threshold trust: Multiple witnesses needed

These mitigations layer defense in depth; no single spoofing technique
defeats all of them.


Role Downgrade Attacks
----------------------

An attacker could force nodes to operate at lower roles:

  * Signed role declarations: Can't forge
  * Local override: Node controls own role
  * Audit trail: Role changes are logged

A node's sovereignty over its own role prevents external actors from
dictating its participation level.


------------------------------------------------------------------------
MEMBERSHIP LIFECYCLE
------------------------------------------------------------------------

Roles define what a node can do; membership defines who belongs. This
section specifies the full lifecycle: enrollment, persistence, voluntary
departure, and involuntary removal.

Implemented in auto-enroll.sls (1280 lines, 41 tests in
test-membership.sps).


Join Policy
-----------

A realm's join policy determines how new members are admitted. Four
policies, from most to least permissive:


  Policy      Description                               When                                         
  open        Any node may join; master auto-approves   Development, testing, personal realms        
  sponsored   Existing member vouches for joiner        Default for small realms (2-10 nodes)        
  voted       N-of-M existing members must approve      Production realms, high-trust environments   
  closed      No new members accepted                   Frozen realms, archival configurations       

The policy is a realm-level setting, persisted in
.vault/membership-state.sexp and enforced by the join listener's
handle-join-connection.


    ;; REPL interface
    (set-join-policy! 'voted 'threshold: '(2 3))
    (realm-join-policy)  ; => voted

The join listener gates every request through a policy check. Revoked
principals are rejected first, regardless of policy. Under closed, all
requests are rejected. Under voted, requests enter the pending queue.
Under sponsored, the sponsoring member's approval auto-satisfies the (1
1) threshold. Under open, the original auto-approve behavior applies.


    ;; Join policy enforcement (handle-join-connection)
    (cond
      ((principal-revoked? node-name)
       (enrollment-send out '(join-rejected ...)))
      ((eq? *join-policy* 'closed)
       (enrollment-send out '(join-rejected ...)))
      ((eq? *join-policy* 'voted)
       (propose-join node-name pubkey hardware)
       (enrollment-send out '(join-pending ...)))
      ((eq? *join-policy* 'sponsored)
       (propose-join node-name pubkey hardware)
       ;; auto-approves: threshold (1 1)
       (enrollment-send out '(join-accepted ...)))
      (else ;; open
       (create-enrollment-cert ...)
       (enrollment-send out '(join-accepted ...))))


Sponsored Enrollment
--------------------

Under the sponsored policy, the sponsoring member's identity is recorded
in the enrollment certificate:


    (signed-enrollment-cert
      (spki-cert
        (issuer (principal ed25519:...))
        (subject (name new-node) (principal ed25519:...))
        (role full)
        (sponsor fluffy)             ; who vouched
        (validity (not-before ...) (not-after ...))))
      (signature ...))

The sponsor field creates an accountability chain. If a sponsored node
misbehaves, the sponsor's judgment is part of the audit record. Under
the sponsored policy, propose-join creates a proposal with threshold (1
1), which auto-approves immediately since the proposer's vote satisfies
the requirement.


Voted Enrollment
----------------

Under the voted policy, a join request enters a pending queue. Existing
members vote to approve or reject. The actual proposal structure as
implemented:


    ;; Proposal structure (as stored in *pending-proposals*)
    (proposal
      (id "7368620C2C79...")       ; SHA-256 of type:subject:timestamp
      (type join)                    ; join | disbar
      (subject starlight)
      (pubkey #vu8(...))
      (hardware (introspection ...))
      (proposed-by fluffy)
      (proposed-at 1770583600)
      (votes ((fluffy . approve)
              (luna . approve)))
      (threshold (2 3))              ; need 2-of-3
      (expires 1771188400)           ; proposed-at + 604800 (7 days)
      (status pending))              ; pending | approved | rejected | expired

Proposal IDs are SHA-256 hashes of type:subject:timestamp, providing
collision-free identifiers suitable for gossip convergence.

When the threshold is met, check-threshold! calls execute-proposal!,
which has the master issue the enrollment certificate. When enough
rejections make approval impossible, the proposal is marked rejected.

Pending proposals expire after a configurable timeout (default: 7 days,
*proposal-ttl*). expire-proposals! garbage-collects stale entries on
every queue access.


    ;; REPL interface
    (propose-join 'new-node pubkey hardware)
    (vote-proposal "7368620C2C79..." 'approve)  ; or 'reject
    (pending-proposals)  ; list all pending
    (review-proposals)   ; interactive display with voting instructions


Enrollment Persistence
----------------------

After successful enrollment (by any policy), four artifacts are
persisted to the vault:


    .vault/
      certs/membership.sexp           ; signed enrollment certificate
      keystore/
        enrollment.pub                ; Ed25519 public key (plaintext)
        enrollment.key                ; Ed25519 private key (plaintext)
      realm-state.sexp                ; master, role, members, timestamp
      membership-state.sexp           ; join policy, proposals, revocations

The membership-state.sexp file persists the join policy, vote threshold,
pending proposals, and revocation list:


    (membership-state
      (version 1)
      (join-policy voted)
      (vote-threshold (2 3))
      (proposals (...))
      (revocation-list (...))
      (timestamp 1770920000))

On restart, restore-realm-state calls load-membership-state!, which
restores the join policy, proposals, and revocation list. Stale
proposals are expired immediately on load.

The system checks three enrollment artifacts (cert, keypair,
realm-state). If any is missing or invalid, the node falls back to fresh
auto-enrollment. This three-point check prevents a node from operating
with stale identity material.

Hardware capabilities and scaling factors are NOT persisted. They are
recomputed from fresh introspection on every startup, ensuring the
node's declared capabilities always match reality.


Startup Notification
--------------------

On restore, pending proposals are displayed to the operator:


    *** 2 pending proposals awaiting your vote ***
      JOIN alice (by fluffy, 3h ago, 1/2 votes) [32FBFF0F75F5]
      DISBAR eve  (by fluffy, 1d ago, 1/2 votes) [489142FCCD98]
    Use (review-proposals) to review and vote.

This ensures that a node returning from downtime immediately sees what
decisions need its attention, rather than silently ignoring pending
membership actions.


------------------------------------------------------------------------
LEAVING A REALM
------------------------------------------------------------------------

A node may voluntarily depart a realm. Departure is clean: the node
revokes its own membership, notifies peers, and returns to the
Wilderness.


Voluntary Departure
-------------------

Implemented in leave-realm. Each step is wrapped in guard for crash
safety:


    (define (leave-realm)
      (unless *my-name*
        (error 'leave-realm "not enrolled in any realm"))
      (let ((name *my-name*)
            (was-master (eq? *my-role* 'master)))
        ;; 1. Revoke local membership cert
        (guard (exn [#t ...]) (revoke-membership!))
        ;; 2. Stop join listener
        (guard (exn [#t #f]) (stop-join-listener))
        ;; 3. Unregister from Bonjour
        (guard (exn [#t #f]) (bonjour-unregister))
        ;; 4. Reset in-memory state
        (set! *realm-master* #f)
        (set! *realm-members* '())
        (set! *my-role* #f) ...             ; all state variables
        `((departed . ,name)
          (was-master . ,was-master))))

Each step is idempotent. A crashed node that restarts after partial
departure will detect the missing cert and fall through to fresh
enrollment, achieving the same end state.


Member List Update
------------------

When a node departs, the remaining members must update their member
lists. The departure is gossiped as a membership event:


    (membership-event
      (type departure)
      (node starlight)
      (timestamp 1770583600)
      (reason voluntary)             ; voluntary | timeout | disbarred
      (signed-by starlight))

On receiving a departure event, each member removes the node from its
local member list and recomputes scaling factors. If the departing node
was master, the remaining members trigger a new election.


Master Departure
----------------

If the master departs, the realm needs a new one. The disbar-member!
procedure already handles this case: when the disbarred or departed node
was master, it triggers elect-master on the remaining members and
assigns the winner as the new master.

If no members remain, the realm ceases to exist. Its artifacts persist
in vaults but no active realm operates.


------------------------------------------------------------------------
DISBARMENT
------------------------------------------------------------------------

Involuntary removal of a malicious or compromised node. Unlike voluntary
departure, the node does not cooperate.


Grounds for Disbarment
----------------------

  * Certificate compromise: Private key leaked or stolen
  * Byzantine behavior: Node issues conflicting statements
  * Resource abuse: Excessive storage, bandwidth, or compute consumption
  * Protocol violation: Malformed messages, replay attacks

Disbarment is a serious action. The bar is high because false positives
destroy trust in the system.


Disbarment Protocol
-------------------

Disbarment always requires a vote, even under open join policy. The
threshold is computed as majority of current members (minimum 2).
propose-disbar creates the proposal; members vote via vote-proposal with
the 'disbar vote type.


    ;; REPL interface
    (propose-disbar 'compromised-node "Certificate compromise"
                    'evidence: "audit-hash-abc123")
    (vote-proposal "DA95EEFC0289..." 'disbar)
    ;; Disbarment proposal (as stored)
    (proposal
      (id "DA95EEFC028B...")
      (type disbar)
      (subject compromised-node)
      (proposed-by fluffy)
      (proposed-at 1770583600)
      (reason "Certificate compromise detected")
      (evidence "audit-hash-abc123")
      (votes ((fluffy . disbar)
              (luna . disbar)))
      (threshold (2 3))
      (expires 1771188400)
      (status pending))

When the threshold is met, disbar-member! executes:

  * The node is added to the revocation list (append-only)
  * The node is removed from the member list
  * Scaling factors are recomputed without the disbarred node
  * If the disbarred node was master, elect-master triggers re-election
  * The revocation list is persisted to .vault/membership-state.sexp


Certificate Revocation
----------------------

Revoked principals are stored in an in-memory list, persisted to
membership-state.sexp, and checked on every join attempt:


    ;; Revocation list entry (as stored)
    ;; (principal timestamp reason revoked-by)
    (eve 1770583600 "Byzantine behavior" (fluffy luna))
    ;; Query interface
    (revocation-list)             ; => list of all entries
    (principal-revoked? 'eve)     ; => #t

Any node encountering a revoked principal in a join request rejects it
immediately, before checking join policy. The revocation list is
append-only and persisted across restarts.


Disbarred Node Behavior
-----------------------

A disbarred node finds itself unable to participate:

  * Join requests are rejected (principal-revoked? check in handle-join-connection)
  * Gossip messages from the node are dropped
  * The node can still operate locally but cannot federate

The node effectively returns to the Wilderness but with a tainted
identity. It must generate new keys to re-enroll, and even then, the
voted policy provides a gate.


------------------------------------------------------------------------
PENDING PROPOSALS QUEUE
------------------------------------------------------------------------

The pending proposals queue (*pending-proposals*) tracks join and
disbarment votes in progress. Persisted to .vault/membership-state.sexp.


Queue State
-----------


    (define *pending-proposals* '())      ; list of proposal s-expressions
    (define *join-policy* 'open)          ; open | sponsored | voted | closed
    (define *vote-threshold* '(2 3))      ; n-of-m (voted policy)
    (define *revocation-list* '())        ; ((principal timestamp reason by) ...)
    (define *proposal-ttl* 604800)        ; 7 days in seconds


Queue Operations
----------------


    ;; Propose a new member (requires sponsored or voted policy)
    (propose-join 'new-node pubkey hardware)
    ;; Propose disbarment (always requires vote, regardless of policy)
    (propose-disbar 'bad-node "reason" 'evidence: "hash")
    ;; Vote on a pending proposal
    (vote-proposal proposal-id 'approve)  ; or 'reject or 'disbar
    ;; List pending proposals
    (pending-proposals)
    ;; Interactive review with voting instructions
    (review-proposals)

Votes are idempotent: a member can change their vote by voting again,
but each member has exactly one vote per proposal. check-threshold! runs
after every vote, triggering execute-proposal! when the approval
threshold is met, or marking the proposal rejected when enough reject
votes make approval impossible.


Interactive Review
------------------

review-proposals displays all pending proposals with full context and
voting instructions. On startup, restore-realm-state shows a summary of
pending proposals:


    ;; Full interactive review
    > (review-proposals)
    === Pending Proposals (2) ===
    Join policy: voted
      Proposal: 7368620C2C79...
      Type:     Join
      Subject:  starlight
      Proposed: fluffy (3h ago)
      Threshold: 2 of 3
      Expires:  6d remaining
      Status:   pending
      Votes:
        fluffy: approve
      (You have not voted)
    To vote: (vote-proposal "7368620C2C79..." 'approve)
         or: (vote-proposal "7368620C2C79..." 'reject)

format-proposal displays a single proposal in detail.
format-proposal-oneline produces the compact summary used in startup
notifications.


Consistency
-----------

Proposals and votes are gossiped, so all members eventually see the same
state. Because votes are idempotent (a member can only vote once per
proposal), convergence is guaranteed regardless of message ordering.

In the event of a network partition, each partition may independently
reach a threshold if enough members are present. When the partition
heals, the gossiped results converge. If conflicting decisions were made
(one partition approved, another rejected), the earlier timestamp wins.


------------------------------------------------------------------------
VOTING PROTOCOL
------------------------------------------------------------------------

Membership votes use threshold-based approval. The specific application
to membership decisions:


Threshold Configuration
-----------------------

The vote threshold is a pair (n m) where n approvals out of m members
are required. Configured via set-join-policy!:


    ;; Set voted policy with 2-of-3 threshold
    (set-join-policy! 'voted 'threshold: '(2 3))
    ;; For disbarment, threshold is computed automatically:
    ;; max(2, ceiling(member-count / 2)) of member-count
    ;; This ensures disbarment always requires a meaningful quorum.

The threshold is a realm-level setting, persisted in
membership-state.sexp. Changing the threshold requires calling
set-join-policy! again.


Vote Processing
---------------

check-threshold! runs after every vote and determines the outcome:

  * Approved: approval count >= n (triggers execute-proposal!)
  * Rejected: rejection count > (m - n) (makes approval impossible)
  * Pending: neither condition met, awaiting more votes

For join proposals, the vote types are 'approve and 'reject. For
disbarment proposals, the vote type is 'disbar (functionally equivalent
to approve).


Private Ballot
--------------

For sensitive decisions (especially disbarment), the homomorphic voting
protocol from Memo-038 could apply. Individual votes would be encrypted;
only the tally revealed.

Private ballot is not yet implemented. The current implementation uses
open voting where all members can see each other's votes. This is
acceptable for small realms where social transparency is appropriate.


------------------------------------------------------------------------
REFERENCES
------------------------------------------------------------------------

  * Memo-010: Federation Protocol
  * Memo-011: Byzantine Consensus
  * Memo-016: Lazy Clustering
  * Memo-017: Security Considerations
  * Memo-038: Quorum Protocol with Homomorphic Voting
  * Memo-050: The Wilderness


------------------------------------------------------------------------
CHANGELOG
------------------------------------------------------------------------

  * 2026-02-09: Reflect implementation in memo (actual code, persistence, interactive review)
  * 2026-02-09: Membership lifecycle (enrollment, departure, disbarment, voting)
  * 2026-01-07: Initial draft (roles and capabilities)

------------------------------------------------------------------------
