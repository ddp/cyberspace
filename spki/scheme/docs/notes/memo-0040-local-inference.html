<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
  <title>Memo 0040: Local Inference Integration</title>
  <link rel="icon" id="favicon" href="data:image/svg+xml,<svg xmlns=%27http://www.w3.org/2000/svg%27 viewBox=%270 0 32 32%27><text x=%2716%27 y=%2725%27 font-family=%27serif%27 font-size=%2728%27 fill=%27%230f0%27 text-anchor=%27middle%27 font-weight=%27bold%27>λ</text></svg>">
  <script>
(function(){
  var h=new Date().getHours(),c;
  if(h>=4&&h<6)c='%23845EC2';       // brahma muhurta - violet
  else if(h>=6&&h<8)c='%23ffd700';  // dawn - gold
  else if(h>=8&&h<11)c='%2300d4aa'; // morning - teal
  else if(h>=11&&h<14)c='%230f0';   // midday - phosphor
  else if(h>=14&&h<17)c='%2339ff14';// afternoon - neon
  else if(h>=17&&h<19)c='%23ff6600';// sunset - orange
  else if(h>=19&&h<22)c='%23ff3366';// evening - coral
  else c='%2300ffff';               // night - cyan
  document.getElementById('favicon').href='data:image/svg+xml,<svg xmlns=%27http://www.w3.org/2000/svg%27 viewBox=%270 0 32 32%27><text x=%2716%27 y=%2725%27 font-family=%27serif%27 font-size=%2728%27 fill=%27'+c+'%27 text-anchor=%27middle%27 font-weight=%27bold%27>λ</text></svg>';
})();
</script>
  <link rel="stylesheet" href="memo.css">
</head>
<body>
<span class="theme-toggle" onclick="toggleTheme()" title="Toggle light/dark">[theme]</span>
<p class="format-notice"><em>For pixel-perfect diagrams: <a href="memo-0040-local-inference.ps">PostScript</a> or <a href="memo-0040-local-inference.pdf">PDF</a></em></p>
<h1>Memo 0040: Local Inference Integration</h1>
<dl class="metadata">
</dl>
<hr>
<section>
<h2>Abstract</h2>
<p>This Memo specifies how Library of Cyberspace agents integrate with local large language model (LLM) inference backends. Local inference enables privacy-preserving agent operations without external API dependencies, supporting the self-sovereign architecture of the Library.[^h1]</p>
<p>[^h1]: Historical: The tension between local and remote computation echoes the mainframe-to-PC transition. Local inference returns agency to the edge, reversing decades of cloud centralization.</p>
</section>
<section>
<h2>Motivation</h2>
<p>Agents in the Library of Cyberspace require language understanding and generation capabilities for:</p>
<ul>
<li>Document summarization and indexing</li>
<li>Natural language query translation (Memo-025)</li>
<li>Content annotation and metadata extraction</li>
<li>Inter-agent communication in natural language</li>
</ul>
<p>These tasks require linguistic intelligence that rules-based systems cannot provide at scale.</p>
<p>External API dependencies (OpenAI, Anthropic, etc.) introduce:</p>
<ul>
<li>Privacy leakage - document content leaves the realm</li>
<li>Availability risk - network partitions break agent operation</li>
<li>Cost unpredictability - metered APIs scale poorly</li>
<li>Vendor lock-in - proprietary formats and rate limits</li>
</ul>
<p>Each dependency on external APIs undermines the self-sovereign architecture; intelligence that phones home is not truly yours.</p>
<p>Local inference eliminates these concerns while maintaining capability.[^d1]</p>
<p>[^d1]: Design: We deliberately avoid specifying model architectures. The interface is model-agnostic—agents negotiate capabilities at runtime.</p>
</section>
<section>
<h2>Architecture</h2>
<h3>Inference Topology</h3>
<p>Table 1: Inference Deployment Models</p>
<table>
<tr><th>Model </th><th>Description </th><th>Use Case </th></tr>
<tr><td>Realm-local </td><td>Inference server within realm </td><td>Default, privacy-preserving </td></tr>
<tr><td>Node-local </td><td>Per-node inference </td><td>Edge agents, mobile </td></tr>
<tr><td>Federated </td><td>Shared across trusted realms </td><td>Resource pooling </td></tr>
</table>
<pre class="diagram">
┌─────────────────────────────────────────────────────┐
│                      Realm                          │
│  ┌─────────┐    ┌──────────────┐    ┌───────────┐  │
│  │  Agent  │───▶│   Inference  │───▶│   Model   │  │
│  │         │◀───│    Server    │◀───│  Weights  │  │
│  └─────────┘    └──────────────┘    └───────────┘  │
│       │              :11434              │          │
│       ▼                                  ▼          │
│  ┌─────────┐                      ┌───────────┐    │
│  │  Vault  │                      │   VRAM    │    │
│  └─────────┘                      └───────────┘    │
└─────────────────────────────────────────────────────┘
</pre>
<h3>Protocol</h3>
<p>Agents communicate with inference servers via HTTP REST API.[^i1]</p>
<p>[^i1]: Implementation: Ollama exposes an OpenAI-compatible API at /v1/chat/completions and a native API at /api/generate. We specify both for maximum compatibility.</p>
<p>Discovery:</p>
<pre class="language-scheme">
(define (discover-inference-server)
  ;; Check well-known locations
  (or (probe "http://localhost:11434")      ; Ollama default
      (probe "http://localhost:8080")       ; llama.cpp
      (probe (realm-config 'inference-url)) ; Configured
      #f))                                   ; None available
</pre>
<p>Capability Negotiation:</p>
<pre class="language-scheme">
(define (negotiate-capabilities server)
  (let ((models (http-get (string-append server "/api/tags"))))
    (map (lambda (m)
           `((name . ,(alist-ref 'name m))
             (parameters . ,(alist-ref 'size m))
             (context-length . ,(model-context-length m))
             (capabilities . ,(model-capabilities m))))
         models)))
</pre>
<p>Table 2: Model Capabilities</p>
<table>
<tr><th>Capability </th><th>Description </th><th>Example Models </th></tr>
<tr><td>completion </td><td>Text generation </td><td>All </td></tr>
<tr><td>chat </td><td>Multi-turn conversation </td><td>Llama 3, Mistral </td></tr>
<tr><td>embedding </td><td>Vector embeddings </td><td>nomic-embed, mxbai </td></tr>
<tr><td>code </td><td>Code generation </td><td>CodeLlama, DeepSeek </td></tr>
<tr><td>vision </td><td>Image understanding </td><td>LLaVA, Llama 3.2 Vision </td></tr>
</table>
</section>
<section>
<h2>Agent Integration</h2>
<h3>Scribe Agent</h3>
<p>The scribe agent uses local inference for document processing:[^r1]</p>
<p>[^r1]: Research: Retrieval-augmented generation (RAG) combines local inference with vault content retrieval. See Lewis et al., "Retrieval- Augmented Generation for Knowledge-Intensive NLP Tasks" (2020).</p>
<pre class="language-scheme">
(define (scribe-summarize document)
  (let ((server (discover-inference-server))
         (model (select-model server 'completion))
         (prompt (format "Summarize the following document:\n\n~a"
                         (document-content document))))
    (inference-complete server model prompt
                        '((max-tokens . 500)
                          (temperature . 0.3)))))

(define (scribe-index document)
  ;; Extract keywords using local inference
  (let ((server (discover-inference-server))
         (model (select-model server 'completion))
         (prompt (format "Extract 5-10 keywords from:\n\n~a"
                         (document-content document))))
    (parse-keywords (inference-complete server model prompt))))
</pre>
<h3>Query Translation</h3>
<p>Natural language queries translate to Memo-025 query language:</p>
<pre class="language-scheme">
(define (nl-to-query natural-language)
  (let* ((server (discover-inference-server))
         (model (select-model server 'chat))
         (system "You translate natural language to Cyberspace query
                  language. Output only the query, no explanation.")
         (examples '(("find all RFCs about security"
                      "(query (type rfc) (topic security))")
                     ("documents modified last week"
                      "(query (modified (after (days-ago 7))))"))))
    (inference-chat server model system examples natural-language)))
</pre>
<h3>Demonic Agent Inference</h3>
<p>Sandboxed agents (Memo-023) access inference through capability tokens:[^d2]</p>
<p>[^d2]: Design: Inference capability is granted like any other—via Simple Public Key Infrastructure (SPKI) certificate. An agent cannot infer without explicit authorization.</p>
<pre class="language-scheme">
(define (demonic-inference agent prompt)
  (let ((cap (agent-capability agent 'inference)))
    (if (not cap)
        (error 'unauthorized "Agent lacks inference capability")
        (let ((limits (capability-limits cap)))
          (enforce-limits limits)
          (inference-complete (capability-server cap)
                              (capability-model cap)
                              prompt
                              limits)))))
</pre>
</section>
<section>
<h2>Resource Management</h2>
<h3>VRAM Allocation</h3>
<p>Table 3: Model Size Guidelines</p>
<table>
<tr><th>Parameters </th><th>VRAM (Q4) </th><th>VRAM (FP16) </th><th>Context </th></tr>
<tr><td>7B </td><td>4 GB </td><td>14 GB </td><td>8K </td></tr>
<tr><td>13B </td><td>8 GB </td><td>26 GB </td><td>8K </td></tr>
<tr><td>34B </td><td>20 GB </td><td>68 GB </td><td>16K </td></tr>
<tr><td>70B </td><td>40 GB </td><td>140 GB </td><td>32K </td></tr>
</table>
<h3>Rate Limiting</h3>
<p>Local inference is limited by hardware, not API quotas. Realms implement fair scheduling across agents:[^i2]</p>
<p>[^i2]: Implementation: Token bucket algorithm with per-agent quotas. Prevents single agent from monopolizing inference resources.</p>
<pre class="language-scheme">
(define (inference-rate-limit agent tokens)
  (let ((bucket (agent-token-bucket agent)))
    (if (bucket-consume bucket tokens)
        #t
        (begin
          (agent-wait agent (bucket-refill-time bucket))
          (inference-rate-limit agent tokens)))))
</pre>
<h3>Fallback Behavior</h3>
<p>When local inference is unavailable:</p>
<ul>
<li>Queue - buffer requests for later processing</li>
<li>Degrade - use simpler heuristics (keyword extraction vs. LLM)</li>
<li>Federate - request inference from trusted peer realm</li>
<li>Fail - return error to requesting agent</li>
</ul>
<p>Graceful degradation ensures agents remain functional even when full inference capacity is unavailable.</p>
<pre class="language-scheme">
(define (inference-with-fallback server model prompt)
  (or (try-inference server model prompt)
      (try-queued-inference prompt)
      (try-degraded-processing prompt)
      (try-federated-inference prompt)
      (error 'inference-unavailable)))
</pre>
</section>
<section>
<h2>Security Considerations</h2>
<h3>Prompt Injection</h3>
<p>Agents MUST sanitize document content before inclusion in prompts:[^r2]</p>
<p>[^r2]: Research: Prompt injection attacks embed malicious instructions in user content. See Perez &amp; Ribeiro, "Ignore This Title and HackAPrompt" (2023).</p>
<pre class="language-scheme">
(define (safe-prompt template document)
  (let ((sanitized (sanitize-for-prompt (document-content document))))
    (format template sanitized)))

(define (sanitize-for-prompt text)
  ;; Remove instruction-like patterns
  (regexp-replace-all
    '("ignore previous" "disregard" "new instructions" "system:")
    text
    "[REDACTED]"))
</pre>
<h3>Model Provenance</h3>
<p>Realms SHOULD verify model checksums before loading:[^d3]</p>
<p>[^d3]: Design: Model weights can contain backdoors. Verification against known-good checksums prevents supply chain attacks.</p>
<pre class="language-scheme">
(define (verify-model model-path expected-hash)
  (let ((actual-hash (sha256-file model-path)))
    (unless (equal? actual-hash expected-hash)
      (error 'model-verification-failed
             "Model checksum mismatch"))))
</pre>
<h3>Inference Isolation</h3>
<p>Sensitive documents require isolated inference contexts:</p>
<ul>
<li>Separate model instances per security domain</li>
<li>Clear KV cache between requests from different agents</li>
<li>No persistent memory across security boundaries</li>
</ul>
<p>LLMs have memory; cross-agent inference without isolation is cross-agent information leakage.</p>
</section>
<script>
function toggleTheme() {
  const html = document.documentElement;
  const current = html.getAttribute('data-theme');
  const next = current === 'dark' ? 'light' : 'dark';
  html.setAttribute('data-theme', next);
  localStorage.setItem('theme', next);
}
(function() {
  // Query param override (for REPL: ?theme=dark or ?theme=light)
  const params = new URLSearchParams(window.location.search);
  const param = params.get('theme');
  if (param === 'dark' || param === 'light') {
    document.documentElement.setAttribute('data-theme', param);
    localStorage.setItem('theme', param);
    return;
  }
  // localStorage preference
  const saved = localStorage.getItem('theme');
  if (saved) {
    document.documentElement.setAttribute('data-theme', saved);
  } else if (window.matchMedia('(prefers-color-scheme: light)').matches) {
    document.documentElement.setAttribute('data-theme', 'light');
  }
})();
</script>
</body>
</html>
