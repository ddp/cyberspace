Memo 0040: Local Inference Integration


------------------------------------------------------------------------

------------------------------------------------------------------------
ABSTRACT
------------------------------------------------------------------------

This Memo specifies how Library of Cyberspace agents integrate with
local large language model (LLM) inference backends. Local inference
enables privacy-preserving agent operations without external API
dependencies, supporting the self-sovereign architecture of the
Library.[^h1]

[^h1]: Historical: The tension between local and remote computation
echoes the mainframe-to-PC transition. Local inference returns agency to
the edge, reversing decades of cloud centralization.


------------------------------------------------------------------------
MOTIVATION
------------------------------------------------------------------------

Agents in the Library of Cyberspace require language understanding and
generation capabilities for:

  * Document summarization and indexing
  * Natural language query translation (Memo-025)
  * Content annotation and metadata extraction
  * Inter-agent communication in natural language

These tasks require linguistic intelligence that rules-based systems
cannot provide at scale.

External API dependencies (OpenAI, Anthropic, etc.) introduce:

  * Privacy leakage - document content leaves the realm
  * Availability risk - network partitions break agent operation
  * Cost unpredictability - metered APIs scale poorly
  * Vendor lock-in - proprietary formats and rate limits

Each dependency on external APIs undermines the self-sovereign
architecture; intelligence that phones home is not truly yours.

Local inference eliminates these concerns while maintaining
capability.[^d1]

[^d1]: Design: We deliberately avoid specifying model architectures. The
interface is model-agnostic—agents negotiate capabilities at runtime.


------------------------------------------------------------------------
ARCHITECTURE
------------------------------------------------------------------------


Inference Topology
------------------

Table 1: Inference Deployment Models


  Model         Description                     Use Case                      
  Realm-local   Inference server within realm   Default, privacy-preserving   
  Node-local    Per-node inference              Edge agents, mobile           
  Federated     Shared across trusted realms    Resource pooling              


    +-----------------------------------------------------+
    |                      Realm                          |
    |  +---------+    +--------------+    +-----------+  |
    |  |  Agent  |--->|   Inference  |--->|   Model   |  |
    |  |         |<---|    Server    |<---|  Weights  |  |
    |  +---------+    +--------------+    +-----------+  |
    |       |              :11434              |          |
    |       v                                  v          |
    |  +---------+                      +-----------+    |
    |  |  Vault  |                      |   VRAM    |    |
    |  +---------+                      +-----------+    |
    +-----------------------------------------------------+


Protocol
--------

Agents communicate with inference servers via HTTP REST API.[^i1]

[^i1]: Implementation: Ollama exposes an OpenAI-compatible API at
/v1/chat/completions and a native API at /api/generate. We specify both
for maximum compatibility.

Discovery:


    (define (discover-inference-server)
      ;; Check well-known locations
      (or (probe "http://localhost:11434")      ; Ollama default
          (probe "http://localhost:8080")       ; llama.cpp
          (probe (realm-config 'inference-url)) ; Configured
          #f))                                   ; None available

Capability Negotiation:


    (define (negotiate-capabilities server)
      (let ((models (http-get (string-append server "/api/tags"))))
        (map (lambda (m)
               `((name . ,(alist-ref 'name m))
                 (parameters . ,(alist-ref 'size m))
                 (context-length . ,(model-context-length m))
                 (capabilities . ,(model-capabilities m))))
             models)))

Table 2: Model Capabilities


  Capability   Description               Example Models            
  completion   Text generation           All                       
  chat         Multi-turn conversation   Llama 3, Mistral          
  embedding    Vector embeddings         nomic-embed, mxbai        
  code         Code generation           CodeLlama, DeepSeek       
  vision       Image understanding       LLaVA, Llama 3.2 Vision   


------------------------------------------------------------------------
AGENT INTEGRATION
------------------------------------------------------------------------


Scribe Agent
------------

The scribe agent uses local inference for document processing:[^r1]

[^r1]: Research: Retrieval-augmented generation (RAG) combines local
inference with vault content retrieval. See Lewis et al., "Retrieval-
Augmented Generation for Knowledge-Intensive NLP Tasks" (2020).


    (define (scribe-summarize document)
      (let ((server (discover-inference-server))
             (model (select-model server 'completion))
             (prompt (format "Summarize the following document:\n\n~a"
                             (document-content document))))
        (inference-complete server model prompt
                            '((max-tokens . 500)
                              (temperature . 0.3)))))
    (define (scribe-index document)
      ;; Extract keywords using local inference
      (let ((server (discover-inference-server))
             (model (select-model server 'completion))
             (prompt (format "Extract 5-10 keywords from:\n\n~a"
                             (document-content document))))
        (parse-keywords (inference-complete server model prompt))))


Query Translation
-----------------

Natural language queries translate to Memo-025 query language:


    (define (nl-to-query natural-language)
      (let* ((server (discover-inference-server))
             (model (select-model server 'chat))
             (system "You translate natural language to Cyberspace query
                      language. Output only the query, no explanation.")
             (examples '(("find all RFCs about security"
                          "(query (type rfc) (topic security))")
                         ("documents modified last week"
                          "(query (modified (after (days-ago 7))))"))))
        (inference-chat server model system examples natural-language)))


Demonic Agent Inference
-----------------------

Sandboxed agents (Memo-023) access inference through capability
tokens:[^d2]

[^d2]: Design: Inference capability is granted like any other—via
Simple Public Key Infrastructure (SPKI) certificate. An agent cannot
infer without explicit authorization.


    (define (demonic-inference agent prompt)
      (let ((cap (agent-capability agent 'inference)))
        (if (not cap)
            (error 'unauthorized "Agent lacks inference capability")
            (let ((limits (capability-limits cap)))
              (enforce-limits limits)
              (inference-complete (capability-server cap)
                                  (capability-model cap)
                                  prompt
                                  limits)))))


------------------------------------------------------------------------
RESOURCE MANAGEMENT
------------------------------------------------------------------------


VRAM Allocation
---------------

Table 3: Model Size Guidelines


  Parameters   VRAM (Q4)   VRAM (FP16)   Context   
  7B           4 GB        14 GB         8K        
  13B          8 GB        26 GB         8K        
  34B          20 GB       68 GB         16K       
  70B          40 GB       140 GB        32K       


Rate Limiting
-------------

Local inference is limited by hardware, not API quotas. Realms implement
fair scheduling across agents:[^i2]

[^i2]: Implementation: Token bucket algorithm with per-agent quotas.
Prevents single agent from monopolizing inference resources.


    (define (inference-rate-limit agent tokens)
      (let ((bucket (agent-token-bucket agent)))
        (if (bucket-consume bucket tokens)
            #t
            (begin
              (agent-wait agent (bucket-refill-time bucket))
              (inference-rate-limit agent tokens)))))


Fallback Behavior
-----------------

When local inference is unavailable:

  * Queue - buffer requests for later processing
  * Degrade - use simpler heuristics (keyword extraction vs. LLM)
  * Federate - request inference from trusted peer realm
  * Fail - return error to requesting agent

Graceful degradation ensures agents remain functional even when full
inference capacity is unavailable.


    (define (inference-with-fallback server model prompt)
      (or (try-inference server model prompt)
          (try-queued-inference prompt)
          (try-degraded-processing prompt)
          (try-federated-inference prompt)
          (error 'inference-unavailable)))


------------------------------------------------------------------------
SECURITY CONSIDERATIONS
------------------------------------------------------------------------


Prompt Injection
----------------

Agents MUST sanitize document content before inclusion in prompts:[^r2]

[^r2]: Research: Prompt injection attacks embed malicious instructions
in user content. See Perez & Ribeiro, "Ignore This Title and
HackAPrompt" (2023).


    (define (safe-prompt template document)
      (let ((sanitized (sanitize-for-prompt (document-content document))))
        (format template sanitized)))
    (define (sanitize-for-prompt text)
      ;; Remove instruction-like patterns
      (regexp-replace-all
        '("ignore previous" "disregard" "new instructions" "system:")
        text
        "[REDACTED]"))


Model Provenance
----------------

Realms SHOULD verify model checksums before loading:[^d3]

[^d3]: Design: Model weights can contain backdoors. Verification against
known-good checksums prevents supply chain attacks.


    (define (verify-model model-path expected-hash)
      (let ((actual-hash (sha256-file model-path)))
        (unless (equal? actual-hash expected-hash)
          (error 'model-verification-failed
                 "Model checksum mismatch"))))


Inference Isolation
-------------------

Sensitive documents require isolated inference contexts:

  * Separate model instances per security domain
  * Clear KV cache between requests from different agents
  * No persistent memory across security boundaries

LLMs have memory; cross-agent inference without isolation is cross-agent
information leakage.

------------------------------------------------------------------------
