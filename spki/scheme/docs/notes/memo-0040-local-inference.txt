Memo 0040: Local Inference Integration


------------------------------------------------------------------------

------------------------------------------------------------------------
ABSTRACT
------------------------------------------------------------------------

This Memo specifies how Library of Cyberspace agents integrate with
local large language model (LLM) inference backends. Local inference
enables privacy-preserving agent operations without external API
dependencies, supporting the self-sovereign architecture of the
Library.[^h1]

[^h1]: Historical: The tension between local and remote computation
echoes the mainframe-to-PC transition. Local inference returns agency to
the edge, reversing decades of cloud centralization.


------------------------------------------------------------------------
MOTIVATION
------------------------------------------------------------------------

Agents in the Library of Cyberspace require language understanding and
generation capabilities for:

  * Document summarization and indexing
  * Natural language query translation (Memo-025)
  * Content annotation and metadata extraction
  * Inter-agent communication in natural language

External API dependencies (OpenAI, Anthropic, etc.) introduce:

1. Privacy leakage — document content leaves the realm 2. Availability
risk — network partitions break agent operation 3. Cost
unpredictability — metered APIs scale poorly 4. Vendor lock-in —
proprietary formats and rate limits

Local inference eliminates these concerns while maintaining
capability.[^d1]

[^d1]: Design: We deliberately avoid specifying model architectures. The
interface is model-agnostic—agents negotiate capabilities at runtime.


------------------------------------------------------------------------
ARCHITECTURE
------------------------------------------------------------------------


Inference Topology
------------------

Table 1: Inference Deployment Models


  Model         Description                     Use Case                      
  Realm-local   Inference server within realm   Default, privacy-preserving   
  Node-local    Per-node inference              Edge agents, mobile           
  Federated     Shared across trusted realms    Resource pooling              


    +-----------------------------------------------------+
    |                      Realm                          |
    |  +---------+    +--------------+    +-----------+  |
    |  |  Agent  |--->|   Inference  |--->|   Model   |  |
    |  |         |<---|    Server    |<---|  Weights  |  |
    |  +---------+    +--------------+    +-----------+  |
    |       |              :11434              |          |
    |       v                                  v          |
    |  +---------+                      +-----------+    |
    |  |  Vault  |                      |   VRAM    |    |
    |  +---------+                      +-----------+    |
    +-----------------------------------------------------+


Protocol
--------

Agents communicate with inference servers via HTTP REST API.[^i1]

[^i1]: Implementation: Ollama exposes an OpenAI-compatible API at
/v1/chat/completions and a native API at /api/generate. We specify both
for maximum compatibility.

Discovery:


    (define (discover-inference-server)
      ;; Check well-known locations
      (or (probe "http://localhost:11434")      ; Ollama default
          (probe "http://localhost:8080")       ; llama.cpp
          (probe (realm-config 'inference-url)) ; Configured
          #f))                                   ; None available

Capability Negotiation:


    (define (negotiate-capabilities server)
      (let ((models (http-get (string-append server "/api/tags"))))
        (map (lambda (m)
               `((name . ,(alist-ref 'name m))
                 (parameters . ,(alist-ref 'size m))
                 (context-length . ,(model-context-length m))
                 (capabilities . ,(model-capabilities m))))
             models)))

Table 2: Model Capabilities


  Capability   Description               Example Models            
  completion   Text generation           All                       
  chat         Multi-turn conversation   Llama 3, Mistral          
  embedding    Vector embeddings         nomic-embed, mxbai        
  code         Code generation           CodeLlama, DeepSeek       
  vision       Image understanding       LLaVA, Llama 3.2 Vision   


------------------------------------------------------------------------
AGENT INTEGRATION
------------------------------------------------------------------------


Scribe Agent
------------

The scribe agent uses local inference for document processing:[^r1]

[^r1]: Research: Retrieval-augmented generation (RAG) combines local
inference with vault content retrieval. See Lewis et al., "Retrieval-
Augmented Generation for Knowledge-Intensive NLP Tasks" (2020).


    (define (scribe-summarize document)
      (let ((server (discover-inference-server))
             (model (select-model server 'completion))
             (prompt (format "Summarize the following document:\n\n~a"
                             (document-content document))))
        (inference-complete server model prompt
                            '((max-tokens . 500)
                              (temperature . 0.3)))))
    (define (scribe-index document)
      ;; Extract keywords using local inference
      (let ((server (discover-inference-server))
             (model (select-model server 'completion))
             (prompt (format "Extract 5-10 keywords from:\n\n~a"
                             (document-content document))))
        (parse-keywords (inference-complete server model prompt))))


Query Translation
-----------------

Natural language queries translate to Memo-025 query language:


    (define (nl-to-query natural-language)
      (let* ((server (discover-inference-server))
             (model (select-model server 'chat))
             (system "You translate natural language to Cyberspace query
                      language. Output only the query, no explanation.")
             (examples '(("find all RFCs about security"
                          "(query (type rfc) (topic security))")
                         ("documents modified last week"
                          "(query (modified (after (days-ago 7))))"))))
        (inference-chat server model system examples natural-language)))


Demonic Agent Inference
-----------------------

Sandboxed agents (Memo-023) access inference through capability
tokens:[^d2]

[^d2]: Design: Inference capability is granted like any other—via
Simple Public Key Infrastructure (SPKI) certificate. An agent cannot
infer without explicit authorization.


    (define (demonic-inference agent prompt)
      (let ((cap (agent-capability agent 'inference)))
        (if (not cap)
            (error 'unauthorized "Agent lacks inference capability")
            (let ((limits (capability-limits cap)))
              (enforce-limits limits)
              (inference-complete (capability-server cap)
                                  (capability-model cap)
                                  prompt
                                  limits)))))


------------------------------------------------------------------------
RESOURCE MANAGEMENT
------------------------------------------------------------------------


VRAM Allocation
---------------

Table 3: Model Size Guidelines


  Parameters   VRAM (Q4)   VRAM (FP16)   Context   
  7B           4 GB        14 GB         8K        
  13B          8 GB        26 GB         8K        
  34B          20 GB       68 GB         16K       
  70B          40 GB       140 GB        32K       


Rate Limiting
-------------

Local inference is limited by hardware, not API quotas. Realms implement
fair scheduling across agents:[^i2]

[^i2]: Implementation: Token bucket algorithm with per-agent quotas.
Prevents single agent from monopolizing inference resources.


    (define (inference-rate-limit agent tokens)
      (let ((bucket (agent-token-bucket agent)))
        (if (bucket-consume bucket tokens)
            #t
            (begin
              (agent-wait agent (bucket-refill-time bucket))
              (inference-rate-limit agent tokens)))))


Fallback Behavior
-----------------

When local inference is unavailable:

1. Queue — buffer requests for later processing 2. Degrade — use
simpler heuristics (keyword extraction vs. LLM) 3. Federate — request
inference from trusted peer realm 4. Fail — return error to requesting
agent


    (define (inference-with-fallback server model prompt)
      (or (try-inference server model prompt)
          (try-queued-inference prompt)
          (try-degraded-processing prompt)
          (try-federated-inference prompt)
          (error 'inference-unavailable)))


------------------------------------------------------------------------
SECURITY CONSIDERATIONS
------------------------------------------------------------------------


Prompt Injection
----------------

Agents MUST sanitize document content before inclusion in prompts:[^r2]

[^r2]: Research: Prompt injection attacks embed malicious instructions
in user content. See Perez & Ribeiro, "Ignore This Title and
HackAPrompt" (2023).


    (define (safe-prompt template document)
      (let ((sanitized (sanitize-for-prompt (document-content document))))
        (format template sanitized)))
    (define (sanitize-for-prompt text)
      ;; Remove instruction-like patterns
      (regexp-replace-all
        '("ignore previous" "disregard" "new instructions" "system:")
        text
        "[REDACTED]"))


Model Provenance
----------------

Realms SHOULD verify model checksums before loading:[^d3]

[^d3]: Design: Model weights can contain backdoors. Verification against
known-good checksums prevents supply chain attacks.


    (define (verify-model model-path expected-hash)
      (let ((actual-hash (sha256-file model-path)))
        (unless (equal? actual-hash expected-hash)
          (error 'model-verification-failed
                 "Model checksum mismatch"))))


Inference Isolation
-------------------

Sensitive documents require isolated inference contexts:

  * Separate model instances per security domain
  * Clear KV cache between requests from different agents
  * No persistent memory across security boundaries


------------------------------------------------------------------------
OLLAMA INTEGRATION
------------------------------------------------------------------------

Ollama is the reference implementation for local inference.[^h2]

[^h2]: Historical: Ollama follows the Unix philosophy—do one thing
well. It manages models and serves inference. Nothing more.


Installation
------------


    # macOS / Linux
    curl -fsSL https://ollama.com/install.sh | sh
    # Verify
    ollama --version


Model Management
----------------


    # Pull models
    ollama pull llama3
    ollama pull nomic-embed-text
    # List available
    ollama list
    # Remove
    ollama rm unused-model


API Endpoints
-------------

Table 4: Ollama API Endpoints


  Endpoint          Method   Purpose             
  /api/generate     POST     Text completion     
  /api/chat         POST     Multi-turn chat     
  /api/embeddings   POST     Vector embeddings   
  /api/tags         GET      List models         
  /api/show         POST     Model details       


Scheme Bindings
---------------


    (define ollama-base "http://localhost:11434")
    (define (ollama-generate model prompt #!key (options '()))
      (http-post (string-append ollama-base "/api/generate")
                 ((model . ,model)
                   (prompt . ,prompt)
                   (stream . #f)
                   ,@options)))
    (define (ollama-chat model messages #!key (options '()))
      (http-post (string-append ollama-base "/api/chat")
                 ((model . ,model)
                   (messages . ,messages)
                   (stream . #f)
                   ,@options)))
    (define (ollama-embed model text)
      (http-post (string-append ollama-base "/api/embeddings")
                 `((model . ,model)
                   (prompt . ,text))))


------------------------------------------------------------------------
COMPATIBILITY
------------------------------------------------------------------------


Alternative Backends
--------------------

The protocol supports any OpenAI-compatible inference server:

  * llama.cpp — server binary with --host flag - vLLM — production serving with PagedAttention
  * LocalAI — drop-in OpenAI replacement
  * LM Studio — GUI with server mode


Cloud Fallback
--------------

For realms without local GPU resources, cloud inference MAY be used as
fallback with explicit user consent and encryption:


    (define (cloud-inference-fallback prompt)
      (when (user-consent? 'cloud-inference)
        (let ((encrypted (encrypt-for-cloud prompt)))
          ;; Use cloud API with encrypted prompt
          ;; Decrypt response locally
          )))

This is NOT RECOMMENDED for sensitive documents.


------------------------------------------------------------------------
REFERENCES
------------------------------------------------------------------------

1. Memo-023: Demonic Agent Sandboxing 2. Memo-025: Query Language 3.
Memo-035: Mobile Agents and Pub/Sub 4. Ollama Documentation:
https://ollama.com/ 5. Lewis et al., "Retrieval-Augmented Generation"
(2020)


------------------------------------------------------------------------
CHANGELOG
------------------------------------------------------------------------

  * 2026-01-07
  * Initial specification

------------------------------------------------------------------------
